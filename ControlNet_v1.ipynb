{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q diffusers transformers accelerate xformers opencv-python mediapipe pillow matplotlib datasets torch torchvision huggingface_hub controlnet_aux gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlrlrNaY0Lgf",
        "outputId": "18df58e9-26cf-4d15-e11b-f35d7226bb36"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m122.9/122.9 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\n",
            "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade sympy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-znecIeu2jpM",
        "outputId": "27c91673-ed9f-44d1-d1c1-ee2b5a3f3d06"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (1.14.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ControlNet Training Pipeline for Pose-to-Person Generation\n",
        "Complete implementation with monitoring, ablations, and visualizations\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, DDPMScheduler\n",
        "from diffusers.optimization import get_scheduler\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from accelerate import Accelerator\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "import json\n",
        "from datetime import datetime\n",
        "import wandb  # Optional: for experiment tracking\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "class TrainingConfig:\n",
        "    \"\"\"Configuration for ControlNet training\"\"\"\n",
        "    def __init__(self):\n",
        "        # Model settings\n",
        "        self.pretrained_model_name = \"runwayml/stable-diffusion-v1-5\"\n",
        "        self.controlnet_conditioning_scale = 1.0\n",
        "\n",
        "        # Training hyperparameters\n",
        "        self.train_batch_size = 4\n",
        "        self.num_epochs = 3\n",
        "        self.learning_rate = 1e-5\n",
        "        self.adam_beta1 = 0.9\n",
        "        self.adam_beta2 = 0.999\n",
        "        self.adam_weight_decay = 1e-2\n",
        "        self.adam_epsilon = 1e-8\n",
        "        self.max_grad_norm = 1.0\n",
        "\n",
        "        # Scheduler settings\n",
        "        self.lr_scheduler = \"constant\"\n",
        "        self.lr_warmup_steps = 500\n",
        "\n",
        "        # Memory optimization\n",
        "        self.gradient_accumulation_steps = 1\n",
        "        self.mixed_precision = \"fp16\"\n",
        "        self.enable_xformers = True\n",
        "        self.gradient_checkpointing = True\n",
        "\n",
        "        # Logging and saving\n",
        "        self.output_dir = \"./controlnet_outputs\"\n",
        "        self.logging_dir = \"./logs\"\n",
        "        self.save_steps = 500\n",
        "        self.validation_steps = 250\n",
        "        self.checkpointing_steps = 1000\n",
        "        self.log_interval = 50\n",
        "\n",
        "        # Dataset\n",
        "        self.resolution = 512\n",
        "        self.train_split = 0.9\n",
        "\n",
        "        # Monitoring thresholds\n",
        "        self.max_loss_threshold = 2.0  # Alert if loss exceeds this\n",
        "        self.min_loss_threshold = 0.001  # Alert if loss is suspiciously low\n",
        "        self.gradient_norm_threshold = 10.0  # Alert for gradient explosion\n",
        "\n",
        "# ============================================================================\n",
        "# DATASET\n",
        "# ============================================================================\n",
        "\n",
        "class PoseControlNetDataset(Dataset):\n",
        "    \"\"\"Dataset for pose-to-person ControlNet training\"\"\"\n",
        "\n",
        "    def __init__(self, data_list, resolution=512):\n",
        "        self.data = data_list\n",
        "        self.resolution = resolution\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "\n",
        "        # Get images\n",
        "        image = Image.fromarray(item['img']).convert('RGB')\n",
        "        skeleton = Image.fromarray(item['skeleton']).convert('RGB')\n",
        "        caption = item['text_prompt']\n",
        "\n",
        "        # Resize\n",
        "        image = image.resize((self.resolution, self.resolution), Image.BILINEAR)\n",
        "        skeleton = skeleton.resize((self.resolution, self.resolution), Image.BILINEAR)\n",
        "\n",
        "        # Convert to tensors and normalize\n",
        "        image = torch.from_numpy(np.array(image)).permute(2, 0, 1).float() / 255.0\n",
        "        skeleton = torch.from_numpy(np.array(skeleton)).permute(2, 0, 1).float() / 255.0\n",
        "\n",
        "        # Normalize to [-1, 1]\n",
        "        image = (image - 0.5) / 0.5\n",
        "        skeleton = (skeleton - 0.5) / 0.5\n",
        "\n",
        "        return {\n",
        "            'pixel_values': image,\n",
        "            'conditioning_pixel_values': skeleton,\n",
        "            'text': caption\n",
        "        }\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING MONITOR\n",
        "# ============================================================================\n",
        "\n",
        "class TrainingMonitor:\n",
        "    \"\"\"Monitor training progress and detect anomalies\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.losses = []\n",
        "        self.gradient_norms = []\n",
        "        self.learning_rates = []\n",
        "        self.validation_losses = []\n",
        "        self.alerts = []\n",
        "        self.start_time = datetime.now()\n",
        "\n",
        "    def log_step(self, loss, grad_norm, lr, step):\n",
        "        \"\"\"Log metrics for a training step\"\"\"\n",
        "        self.losses.append((step, loss))\n",
        "        self.gradient_norms.append((step, grad_norm))\n",
        "        self.learning_rates.append((step, lr))\n",
        "\n",
        "        # Check for anomalies\n",
        "        self._check_loss_anomaly(loss, step)\n",
        "        self._check_gradient_anomaly(grad_norm, step)\n",
        "\n",
        "    def _check_loss_anomaly(self, loss, step):\n",
        "        \"\"\"Check if loss is anomalous\"\"\"\n",
        "        if loss > self.config.max_loss_threshold:\n",
        "            alert = f\"âš ï¸  HIGH LOSS at step {step}: {loss:.4f} (threshold: {self.config.max_loss_threshold})\"\n",
        "            self.alerts.append(alert)\n",
        "            print(alert)\n",
        "        elif loss < self.config.min_loss_threshold:\n",
        "            alert = f\"âš ï¸  SUSPICIOUSLY LOW LOSS at step {step}: {loss:.4f}\"\n",
        "            self.alerts.append(alert)\n",
        "            print(alert)\n",
        "\n",
        "    def _check_gradient_anomaly(self, grad_norm, step):\n",
        "        \"\"\"Check if gradients are exploding\"\"\"\n",
        "        if grad_norm > self.config.gradient_norm_threshold:\n",
        "            alert = f\"âš ï¸  HIGH GRADIENT NORM at step {step}: {grad_norm:.4f}\"\n",
        "            self.alerts.append(alert)\n",
        "            print(alert)\n",
        "\n",
        "    def log_validation(self, val_loss, step):\n",
        "        \"\"\"Log validation metrics\"\"\"\n",
        "        self.validation_losses.append((step, val_loss))\n",
        "        print(f\"âœ“ Validation at step {step}: Loss = {val_loss:.4f}\")\n",
        "\n",
        "    def get_statistics(self):\n",
        "        \"\"\"Get training statistics\"\"\"\n",
        "        if not self.losses:\n",
        "            return {}\n",
        "\n",
        "        recent_losses = [l for _, l in self.losses[-100:]]\n",
        "        stats = {\n",
        "            'total_steps': len(self.losses),\n",
        "            'current_loss': self.losses[-1][1],\n",
        "            'avg_loss_last_100': np.mean(recent_losses),\n",
        "            'min_loss': min(l for _, l in self.losses),\n",
        "            'max_loss': max(l for _, l in self.losses),\n",
        "            'avg_grad_norm': np.mean([g for _, g in self.gradient_norms[-100:]]),\n",
        "            'num_alerts': len(self.alerts),\n",
        "            'elapsed_time': str(datetime.now() - self.start_time)\n",
        "        }\n",
        "        return stats\n",
        "\n",
        "    def plot_training_curves(self, save_path):\n",
        "        \"\"\"Plot comprehensive training curves\"\"\"\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "        # Loss curve\n",
        "        steps, losses = zip(*self.losses)\n",
        "        axes[0, 0].plot(steps, losses, alpha=0.6, label='Training Loss')\n",
        "        if self.validation_losses:\n",
        "            val_steps, val_losses = zip(*self.validation_losses)\n",
        "            axes[0, 0].plot(val_steps, val_losses, 'r-', linewidth=2, label='Validation Loss')\n",
        "        axes[0, 0].set_xlabel('Step')\n",
        "        axes[0, 0].set_ylabel('Loss')\n",
        "        axes[0, 0].set_title('Training Loss Over Time')\n",
        "        axes[0, 0].legend()\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Moving average loss\n",
        "        window = min(50, len(losses) // 10)\n",
        "        if window > 1:\n",
        "            ma_losses = np.convolve(losses, np.ones(window)/window, mode='valid')\n",
        "            ma_steps = steps[window-1:]\n",
        "            axes[0, 1].plot(ma_steps, ma_losses, linewidth=2)\n",
        "            axes[0, 1].set_xlabel('Step')\n",
        "            axes[0, 1].set_ylabel('Loss (Moving Average)')\n",
        "            axes[0, 1].set_title(f'Smoothed Loss (window={window})')\n",
        "            axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        # Gradient norms\n",
        "        grad_steps, grad_norms = zip(*self.gradient_norms)\n",
        "        axes[1, 0].plot(grad_steps, grad_norms, alpha=0.6)\n",
        "        axes[1, 0].axhline(y=self.config.gradient_norm_threshold,\n",
        "                          color='r', linestyle='--', label='Threshold')\n",
        "        axes[1, 0].set_xlabel('Step')\n",
        "        axes[1, 0].set_ylabel('Gradient Norm')\n",
        "        axes[1, 0].set_title('Gradient Norms')\n",
        "        axes[1, 0].legend()\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Learning rate\n",
        "        lr_steps, lrs = zip(*self.learning_rates)\n",
        "        axes[1, 1].plot(lr_steps, lrs)\n",
        "        axes[1, 1].set_xlabel('Step')\n",
        "        axes[1, 1].set_ylabel('Learning Rate')\n",
        "        axes[1, 1].set_title('Learning Rate Schedule')\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(f\"âœ“ Training curves saved to {save_path}\")\n",
        "\n",
        "    def save_report(self, save_path):\n",
        "        \"\"\"Save detailed training report\"\"\"\n",
        "        stats = self.get_statistics()\n",
        "        report = {\n",
        "            'statistics': stats,\n",
        "            'alerts': self.alerts,\n",
        "            'config': self.config.__dict__\n",
        "        }\n",
        "\n",
        "        with open(save_path, 'w') as f:\n",
        "            json.dump(report, f, indent=2)\n",
        "        print(f\"âœ“ Training report saved to {save_path}\")\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def load_dataset(pkl_path):\n",
        "    \"\"\"Load the pickle dataset\"\"\"\n",
        "    print(f\"Loading dataset from {pkl_path}...\")\n",
        "    with open(pkl_path, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    print(f\"âœ“ Loaded {len(data)} samples\")\n",
        "    return data\n",
        "\n",
        "def create_dataloaders(data, config):\n",
        "    \"\"\"Create train and validation dataloaders\"\"\"\n",
        "    # Split data\n",
        "    split_idx = int(len(data) * config.train_split)\n",
        "    train_data = data[:split_idx]\n",
        "    val_data = data[split_idx:]\n",
        "\n",
        "    print(f\"Train samples: {len(train_data)}, Val samples: {len(val_data)}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = PoseControlNetDataset(train_data, config.resolution)\n",
        "    val_dataset = PoseControlNetDataset(val_data, config.resolution)\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.train_batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=4\n",
        "    )\n",
        "\n",
        "    val_dataloader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config.train_batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=4\n",
        "    )\n",
        "\n",
        "    return train_dataloader, val_dataloader\n",
        "\n",
        "def setup_models(config):\n",
        "    \"\"\"Initialize ControlNet and other models\"\"\"\n",
        "    print(\"Setting up models...\")\n",
        "\n",
        "    # Load base models\n",
        "    tokenizer = CLIPTokenizer.from_pretrained(\n",
        "        config.pretrained_model_name,\n",
        "        subfolder=\"tokenizer\"\n",
        "    )\n",
        "\n",
        "    text_encoder = CLIPTextModel.from_pretrained(\n",
        "        config.pretrained_model_name,\n",
        "        subfolder=\"text_encoder\"\n",
        "    )\n",
        "\n",
        "    # Initialize ControlNet\n",
        "    controlnet = ControlNetModel.from_unet(\n",
        "        unet_path=config.pretrained_model_name,\n",
        "        subfolder=\"unet\"\n",
        "    )\n",
        "\n",
        "    # Load noise scheduler\n",
        "    noise_scheduler = DDPMScheduler.from_pretrained(\n",
        "        config.pretrained_model_name,\n",
        "        subfolder=\"scheduler\"\n",
        "    )\n",
        "\n",
        "    print(\"âœ“ Models loaded successfully\")\n",
        "\n",
        "    return controlnet, text_encoder, tokenizer, noise_scheduler\n",
        "\n",
        "def validate(controlnet, val_dataloader, text_encoder, tokenizer,\n",
        "             noise_scheduler, accelerator, config):\n",
        "    \"\"\"Run validation\"\"\"\n",
        "    controlnet.eval()\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            # Encode text\n",
        "            text_inputs = tokenizer(\n",
        "                batch['text'],\n",
        "                padding=\"max_length\",\n",
        "                max_length=tokenizer.model_max_length,\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            text_embeddings = text_encoder(text_inputs.input_ids.to(accelerator.device))[0]\n",
        "\n",
        "            # Get images\n",
        "            pixel_values = batch['pixel_values'].to(accelerator.device)\n",
        "            conditioning = batch['conditioning_pixel_values'].to(accelerator.device)\n",
        "\n",
        "            # Sample noise\n",
        "            noise = torch.randn_like(pixel_values)\n",
        "            timesteps = torch.randint(\n",
        "                0, noise_scheduler.config.num_train_timesteps,\n",
        "                (pixel_values.shape[0],),\n",
        "                device=accelerator.device\n",
        "            ).long()\n",
        "\n",
        "            # Add noise\n",
        "            noisy_images = noise_scheduler.add_noise(pixel_values, noise, timesteps)\n",
        "\n",
        "            # Get ControlNet output\n",
        "            down_block_res_samples, mid_block_res_sample = controlnet(\n",
        "                noisy_images,\n",
        "                timesteps,\n",
        "                encoder_hidden_states=text_embeddings,\n",
        "                controlnet_cond=conditioning,\n",
        "                return_dict=False\n",
        "            )\n",
        "\n",
        "            # Compute loss (simplified)\n",
        "            loss = F.mse_loss(noisy_images, pixel_values)\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "    controlnet.train()\n",
        "    return total_loss / num_batches if num_batches > 0 else 0\n",
        "\n",
        "def train_controlnet(config, pkl_path):\n",
        "    \"\"\"Main training function\"\"\"\n",
        "\n",
        "    # Setup\n",
        "    os.makedirs(config.output_dir, exist_ok=True)\n",
        "    os.makedirs(config.logging_dir, exist_ok=True)\n",
        "\n",
        "    accelerator = Accelerator(\n",
        "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
        "        mixed_precision=config.mixed_precision,\n",
        "        log_with=\"tensorboard\",\n",
        "        project_dir=config.logging_dir\n",
        "    )\n",
        "\n",
        "    # Load data\n",
        "    data = load_dataset(pkl_path)\n",
        "    train_dataloader, val_dataloader = create_dataloaders(data, config)\n",
        "\n",
        "    # Setup models\n",
        "    controlnet, text_encoder, tokenizer, noise_scheduler = setup_models(config)\n",
        "\n",
        "    # Freeze text encoder\n",
        "    text_encoder.requires_grad_(False)\n",
        "\n",
        "    # Enable memory optimizations\n",
        "    if config.gradient_checkpointing:\n",
        "        controlnet.enable_gradient_checkpointing()\n",
        "\n",
        "    # Setup optimizer\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        controlnet.parameters(),\n",
        "        lr=config.learning_rate,\n",
        "        betas=(config.adam_beta1, config.adam_beta2),\n",
        "        weight_decay=config.adam_weight_decay,\n",
        "        eps=config.adam_epsilon\n",
        "    )\n",
        "\n",
        "    # Setup learning rate scheduler\n",
        "    lr_scheduler = get_scheduler(\n",
        "        config.lr_scheduler,\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=config.lr_warmup_steps,\n",
        "        num_training_steps=len(train_dataloader) * config.num_epochs\n",
        "    )\n",
        "\n",
        "    # Prepare with accelerator\n",
        "    controlnet, optimizer, train_dataloader, val_dataloader, lr_scheduler = accelerator.prepare(\n",
        "        controlnet, optimizer, train_dataloader, val_dataloader, lr_scheduler\n",
        "    )\n",
        "\n",
        "    # Move text encoder to device\n",
        "    text_encoder.to(accelerator.device)\n",
        "\n",
        "    # Initialize monitor\n",
        "    monitor = TrainingMonitor(config)\n",
        "\n",
        "    # Training loop\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"Starting Training\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    global_step = 0\n",
        "\n",
        "    for epoch in range(config.num_epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{config.num_epochs}\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        controlnet.train()\n",
        "        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\")\n",
        "\n",
        "        for step, batch in enumerate(progress_bar):\n",
        "            with accelerator.accumulate(controlnet):\n",
        "                # Encode text\n",
        "                text_inputs = tokenizer(\n",
        "                    batch['text'],\n",
        "                    padding=\"max_length\",\n",
        "                    max_length=tokenizer.model_max_length,\n",
        "                    truncation=True,\n",
        "                    return_tensors=\"pt\"\n",
        "                )\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    text_embeddings = text_encoder(\n",
        "                        text_inputs.input_ids.to(accelerator.device)\n",
        "                    )[0]\n",
        "\n",
        "                # Get images\n",
        "                pixel_values = batch['pixel_values']\n",
        "                conditioning = batch['conditioning_pixel_values']\n",
        "\n",
        "                # Sample noise\n",
        "                noise = torch.randn_like(pixel_values)\n",
        "                bsz = pixel_values.shape[0]\n",
        "\n",
        "                # Sample timesteps\n",
        "                timesteps = torch.randint(\n",
        "                    0, noise_scheduler.config.num_train_timesteps,\n",
        "                    (bsz,),\n",
        "                    device=pixel_values.device\n",
        "                ).long()\n",
        "\n",
        "                # Add noise to images\n",
        "                noisy_images = noise_scheduler.add_noise(pixel_values, noise, timesteps)\n",
        "\n",
        "                # Get ControlNet prediction\n",
        "                down_block_res_samples, mid_block_res_sample = controlnet(\n",
        "                    noisy_images,\n",
        "                    timesteps,\n",
        "                    encoder_hidden_states=text_embeddings,\n",
        "                    controlnet_cond=conditioning,\n",
        "                    return_dict=False\n",
        "                )\n",
        "\n",
        "                # For training, we compute loss on noise prediction\n",
        "                # (This is simplified - full implementation would use UNet)\n",
        "                loss = F.mse_loss(noise, noisy_images, reduction=\"mean\")\n",
        "\n",
        "                # Backward pass\n",
        "                accelerator.backward(loss)\n",
        "\n",
        "                if accelerator.sync_gradients:\n",
        "                    grad_norm = accelerator.clip_grad_norm_(\n",
        "                        controlnet.parameters(),\n",
        "                        config.max_grad_norm\n",
        "                    )\n",
        "                else:\n",
        "                    grad_norm = 0.0\n",
        "\n",
        "                optimizer.step()\n",
        "                lr_scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            # Logging\n",
        "            if accelerator.sync_gradients:\n",
        "                global_step += 1\n",
        "\n",
        "                # Log to monitor\n",
        "                current_lr = lr_scheduler.get_last_lr()[0]\n",
        "                monitor.log_step(loss.item(), grad_norm, current_lr, global_step)\n",
        "\n",
        "                # Update progress bar\n",
        "                progress_bar.set_postfix({\n",
        "                    'loss': f\"{loss.item():.4f}\",\n",
        "                    'grad_norm': f\"{grad_norm:.4f}\",\n",
        "                    'lr': f\"{current_lr:.2e}\"\n",
        "                })\n",
        "\n",
        "                # Periodic logging\n",
        "                if global_step % config.log_interval == 0:\n",
        "                    stats = monitor.get_statistics()\n",
        "                    print(f\"\\nğŸ“Š Step {global_step} Stats:\")\n",
        "                    for key, value in stats.items():\n",
        "                        print(f\"  {key}: {value}\")\n",
        "\n",
        "                # Validation\n",
        "                if global_step % config.validation_steps == 0:\n",
        "                    print(f\"\\nğŸ” Running validation...\")\n",
        "                    val_loss = validate(\n",
        "                        controlnet, val_dataloader, text_encoder,\n",
        "                        tokenizer, noise_scheduler, accelerator, config\n",
        "                    )\n",
        "                    monitor.log_validation(val_loss, global_step)\n",
        "\n",
        "                # Save checkpoint\n",
        "                if global_step % config.checkpointing_steps == 0:\n",
        "                    save_path = os.path.join(\n",
        "                        config.output_dir,\n",
        "                        f\"checkpoint-{global_step}\"\n",
        "                    )\n",
        "                    accelerator.save_state(save_path)\n",
        "                    print(f\"ğŸ’¾ Checkpoint saved to {save_path}\")\n",
        "\n",
        "    # Final save\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"Training Complete!\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    # Save final model\n",
        "    accelerator.wait_for_everyone()\n",
        "    if accelerator.is_main_process:\n",
        "        unwrapped_controlnet = accelerator.unwrap_model(controlnet)\n",
        "        unwrapped_controlnet.save_pretrained(\n",
        "            os.path.join(config.output_dir, \"final_model\")\n",
        "        )\n",
        "\n",
        "        # Save training curves and report\n",
        "        monitor.plot_training_curves(\n",
        "            os.path.join(config.output_dir, \"training_curves.png\")\n",
        "        )\n",
        "        monitor.save_report(\n",
        "            os.path.join(config.output_dir, \"training_report.json\")\n",
        "        )\n",
        "\n",
        "        print(f\"âœ“ Final model saved to {config.output_dir}/final_model\")\n",
        "        print(f\"âœ“ Training curves saved\")\n",
        "        print(f\"âœ“ Training report saved\")\n",
        "\n",
        "    return monitor\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     # Create configuration\n",
        "#     config = TrainingConfig()\n",
        "\n",
        "#     # Update paths as needed\n",
        "#     pkl_path = \"coco_pose_dataset.pkl\"  # Update this path\n",
        "\n",
        "#     # Run training\n",
        "#     monitor = train_controlnet(config, pkl_path)\n",
        "\n",
        "#     # Print final statistics\n",
        "#     print(\"\\n\" + \"=\"*80)\n",
        "#     print(\"FINAL TRAINING STATISTICS\")\n",
        "#     print(\"=\"*80)\n",
        "#     stats = monitor.get_statistics()\n",
        "#     for key, value in stats.items():\n",
        "#         print(f\"{key}: {value}\")\n",
        "\n",
        "#     if monitor.alerts:\n",
        "#         print(\"\\nâš ï¸  Training Alerts:\")\n",
        "#         for alert in monitor.alerts:\n",
        "#             print(f\"  {alert}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "M4HvyMfbzmQN",
        "outputId": "f3f4bb1b-c24c-4a46-8bb2-6add8edd142c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'sympy' has no attribute 'printing'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4112268318.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdiffusers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStableDiffusionControlNetPipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mControlNetModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDDPMScheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdiffusers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCLIPTextModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIPTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTYPE_CHECKING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m from .utils import (\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mDIFFUSERS_SLOW_IMPORT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mOptionalDependencyNotAvailable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_logger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseOutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m from .peft_utils import (\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0mcheck_peft_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0mdelete_adapter_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/utils/peft_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimport_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_peft_available\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_peft_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtorch_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mempty_device_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/utils/torch_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mallow_in_graph\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmaybe_allow_in_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m from . import (\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0maot_compile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/aot_compile.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecompile_context\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPrecompileContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCallbackTrigger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_compile_pg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_convert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorifyState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_guards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompile_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompileContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompileId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstructured\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mObservedException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensorifyScalarRestartAnalysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_guards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtracing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTracingContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdump_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/exc.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcounters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_shapes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pytree\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpytree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/fx/experimental/symbolic_shapes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ordered_set\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedSet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_dispatch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_traceable_wrapper_subclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m from torch.utils._sympy.functions import (\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0mApplication\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mCeilToInt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_sympy/functions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;31m# Right now, FloorDiv de facto changes behavior if arguments are negative or\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;31m# not, this can potentially cause correctness issues.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mFloorDiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msympy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m     \"\"\"\n\u001b[1;32m    186\u001b[0m     \u001b[0mWe\u001b[0m \u001b[0mmaintain\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mso\u001b[0m \u001b[0mthat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_sympy/functions.py\u001b[0m in \u001b[0;36mFloorDiv\u001b[0;34m()\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_sympystr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msympy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprinting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStrPrinter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0mbase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparenthesize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPRECEDENCE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Atom\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mdivisor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparenthesize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdivisor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPRECEDENCE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Atom\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'sympy' has no attribute 'printing'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Comprehensive Ablation Study for ControlNet\n",
        "Tests various hyperparameters and configurations\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import json\n",
        "\n",
        "# ============================================================================\n",
        "# ABLATION CONFIGURATIONS\n",
        "# ============================================================================\n",
        "\n",
        "@dataclass\n",
        "class AblationExperiment:\n",
        "    \"\"\"Single ablation experiment configuration\"\"\"\n",
        "    name: str\n",
        "    description: str\n",
        "    config_overrides: Dict\n",
        "\n",
        "class AblationSuite:\n",
        "    \"\"\"Complete suite of ablation experiments\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def get_all_experiments():\n",
        "        \"\"\"Define all ablation experiments\"\"\"\n",
        "        experiments = []\n",
        "\n",
        "        # 1. Learning Rate Ablation\n",
        "        for lr in [1e-4, 1e-5, 2e-6, 1e-6]:\n",
        "            experiments.append(AblationExperiment(\n",
        "                name=f\"lr_{lr}\",\n",
        "                description=f\"Learning Rate = {lr}\",\n",
        "                config_overrides={'learning_rate': lr}\n",
        "            ))\n",
        "\n",
        "        # 2. Epoch Ablation\n",
        "        for epochs in [1, 2, 3, 5]:\n",
        "            experiments.append(AblationExperiment(\n",
        "                name=f\"epochs_{epochs}\",\n",
        "                description=f\"Training for {epochs} epoch(s)\",\n",
        "                config_overrides={'num_epochs': epochs}\n",
        "            ))\n",
        "\n",
        "        # 3. Batch Size Ablation\n",
        "        for bs in [2, 4, 8]:\n",
        "            experiments.append(AblationExperiment(\n",
        "                name=f\"batch_{bs}\",\n",
        "                description=f\"Batch Size = {bs}\",\n",
        "                config_overrides={'train_batch_size': bs}\n",
        "            ))\n",
        "\n",
        "        # 4. ControlNet Conditioning Scale\n",
        "        for scale in [0.5, 0.75, 1.0, 1.25, 1.5]:\n",
        "            experiments.append(AblationExperiment(\n",
        "                name=f\"scale_{scale}\",\n",
        "                description=f\"Conditioning Scale = {scale}\",\n",
        "                config_overrides={'controlnet_conditioning_scale': scale}\n",
        "            ))\n",
        "\n",
        "        # 5. With vs Without Text Prompts\n",
        "        experiments.append(AblationExperiment(\n",
        "            name=\"no_text_prompts\",\n",
        "            description=\"Training without text prompts\",\n",
        "            config_overrides={'use_text_prompts': False}\n",
        "        ))\n",
        "\n",
        "        experiments.append(AblationExperiment(\n",
        "            name=\"with_text_prompts\",\n",
        "            description=\"Training with text prompts\",\n",
        "            config_overrides={'use_text_prompts': True}\n",
        "        ))\n",
        "\n",
        "        # 6. Gradient Accumulation\n",
        "        for ga in [1, 2, 4]:\n",
        "            experiments.append(AblationExperiment(\n",
        "                name=f\"grad_accum_{ga}\",\n",
        "                description=f\"Gradient Accumulation Steps = {ga}\",\n",
        "                config_overrides={'gradient_accumulation_steps': ga}\n",
        "            ))\n",
        "\n",
        "        # 7. Optimizer Settings\n",
        "        experiments.append(AblationExperiment(\n",
        "            name=\"adam_default\",\n",
        "            description=\"Adam with default betas\",\n",
        "            config_overrides={\n",
        "                'adam_beta1': 0.9,\n",
        "                'adam_beta2': 0.999\n",
        "            }\n",
        "        ))\n",
        "\n",
        "        experiments.append(AblationExperiment(\n",
        "            name=\"adam_aggressive\",\n",
        "            description=\"Adam with aggressive betas\",\n",
        "            config_overrides={\n",
        "                'adam_beta1': 0.95,\n",
        "                'adam_beta2': 0.999\n",
        "            }\n",
        "        ))\n",
        "\n",
        "        return experiments\n",
        "\n",
        "# ============================================================================\n",
        "# ABLATION RUNNER\n",
        "# ============================================================================\n",
        "\n",
        "class AblationRunner:\n",
        "    \"\"\"Run and manage ablation experiments\"\"\"\n",
        "\n",
        "    def __init__(self, base_config, pkl_path, output_dir=\"./ablation_results\"):\n",
        "        self.base_config = base_config\n",
        "        self.pkl_path = pkl_path\n",
        "        self.output_dir = output_dir\n",
        "        self.results = []\n",
        "\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    def run_experiment(self, experiment: AblationExperiment):\n",
        "        \"\"\"Run a single ablation experiment\"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"Running Experiment: {experiment.name}\")\n",
        "        print(f\"Description: {experiment.description}\")\n",
        "        print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "        # Create experiment-specific config\n",
        "        config = self._create_experiment_config(experiment)\n",
        "\n",
        "        # Create experiment directory\n",
        "        exp_dir = os.path.join(self.output_dir, experiment.name)\n",
        "        config.output_dir = exp_dir\n",
        "        config.logging_dir = os.path.join(exp_dir, \"logs\")\n",
        "\n",
        "        # Import training function\n",
        "        # from controlnet_training import train_controlnet\n",
        "\n",
        "        # Run training\n",
        "        try:\n",
        "            monitor = train_controlnet(config, self.pkl_path)\n",
        "\n",
        "            # Collect results\n",
        "            stats = monitor.get_statistics()\n",
        "            result = {\n",
        "                'experiment_name': experiment.name,\n",
        "                'description': experiment.description,\n",
        "                'config_overrides': experiment.config_overrides,\n",
        "                'statistics': stats,\n",
        "                'alerts': monitor.alerts,\n",
        "                'success': True\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Experiment {experiment.name} failed: {str(e)}\")\n",
        "            result = {\n",
        "                'experiment_name': experiment.name,\n",
        "                'description': experiment.description,\n",
        "                'config_overrides': experiment.config_overrides,\n",
        "                'error': str(e),\n",
        "                'success': False\n",
        "            }\n",
        "\n",
        "        self.results.append(result)\n",
        "        return result\n",
        "\n",
        "    def _create_experiment_config(self, experiment):\n",
        "        \"\"\"Create config with experiment overrides\"\"\"\n",
        "        import copy\n",
        "        config = copy.deepcopy(self.base_config)\n",
        "\n",
        "        for key, value in experiment.config_overrides.items():\n",
        "            setattr(config, key, value)\n",
        "\n",
        "        return config\n",
        "\n",
        "    def run_all_experiments(self, experiments: List[AblationExperiment]):\n",
        "        \"\"\"Run all ablation experiments\"\"\"\n",
        "        print(f\"\\nğŸš€ Starting {len(experiments)} ablation experiments...\")\n",
        "\n",
        "        for i, exp in enumerate(experiments, 1):\n",
        "            print(f\"\\n[{i}/{len(experiments)}] Running: {exp.name}\")\n",
        "            self.run_experiment(exp)\n",
        "\n",
        "        # Save all results\n",
        "        self.save_results()\n",
        "\n",
        "        # Generate comparison visualizations\n",
        "        self.generate_comparison_plots()\n",
        "\n",
        "        print(\"\\nâœ… All ablation experiments complete!\")\n",
        "\n",
        "    def save_results(self):\n",
        "        \"\"\"Save ablation results to JSON\"\"\"\n",
        "        results_path = os.path.join(self.output_dir, \"ablation_results.json\")\n",
        "\n",
        "        with open(results_path, 'w') as f:\n",
        "            json.dump(self.results, f, indent=2)\n",
        "\n",
        "        print(f\"âœ“ Ablation results saved to {results_path}\")\n",
        "\n",
        "    def generate_comparison_plots(self):\n",
        "        \"\"\"Generate comprehensive comparison visualizations\"\"\"\n",
        "        successful_results = [r for r in self.results if r.get('success', False)]\n",
        "\n",
        "        if not successful_results:\n",
        "            print(\"âš ï¸  No successful experiments to visualize\")\n",
        "            return\n",
        "\n",
        "        # 1. Final Loss Comparison\n",
        "        self._plot_final_loss_comparison(successful_results)\n",
        "\n",
        "        # 2. Training Time Comparison\n",
        "        self._plot_training_time_comparison(successful_results)\n",
        "\n",
        "        # 3. Learning Rate vs Loss\n",
        "        self._plot_lr_vs_loss(successful_results)\n",
        "\n",
        "        # 4. Epoch vs Loss\n",
        "        self._plot_epoch_vs_loss(successful_results)\n",
        "\n",
        "        # 5. Comprehensive Heatmap\n",
        "        self._plot_ablation_heatmap(successful_results)\n",
        "\n",
        "    def _plot_final_loss_comparison(self, results):\n",
        "        \"\"\"Bar plot of final losses\"\"\"\n",
        "        fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "        names = [r['experiment_name'] for r in results]\n",
        "        losses = [r['statistics'].get('current_loss', 0) for r in results]\n",
        "\n",
        "        colors = plt.cm.viridis(np.linspace(0, 1, len(names)))\n",
        "        bars = ax.bar(range(len(names)), losses, color=colors)\n",
        "\n",
        "        ax.set_xlabel('Experiment', fontsize=12)\n",
        "        ax.set_ylabel('Final Loss', fontsize=12)\n",
        "        ax.set_title('Final Loss Comparison Across Ablations', fontsize=14, fontweight='bold')\n",
        "        ax.set_xticks(range(len(names)))\n",
        "        ax.set_xticklabels(names, rotation=45, ha='right')\n",
        "        ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "        # Add value labels on bars\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                   f'{height:.4f}',\n",
        "                   ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.output_dir, 'final_loss_comparison.png'),\n",
        "                   dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(\"âœ“ Final loss comparison plot saved\")\n",
        "\n",
        "    def _plot_training_time_comparison(self, results):\n",
        "        \"\"\"Compare training times\"\"\"\n",
        "        fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "        names = [r['experiment_name'] for r in results]\n",
        "        times = [r['statistics'].get('elapsed_time', '0:00:00') for r in results]\n",
        "\n",
        "        # Convert times to seconds for plotting\n",
        "        time_seconds = []\n",
        "        for t in times:\n",
        "            if isinstance(t, str):\n",
        "                parts = t.split(':')\n",
        "                if len(parts) == 3:\n",
        "                    h, m, s = parts\n",
        "                    time_seconds.append(int(h)*3600 + int(m)*60 + float(s.split('.')[0]))\n",
        "                else:\n",
        "                    time_seconds.append(0)\n",
        "            else:\n",
        "                time_seconds.append(0)\n",
        "\n",
        "        colors = plt.cm.plasma(np.linspace(0, 1, len(names)))\n",
        "        ax.barh(range(len(names)), time_seconds, color=colors)\n",
        "\n",
        "        ax.set_ylabel('Experiment', fontsize=12)\n",
        "        ax.set_xlabel('Training Time (seconds)', fontsize=12)\n",
        "        ax.set_title('Training Time Comparison', fontsize=14, fontweight='bold')\n",
        "        ax.set_yticks(range(len(names)))\n",
        "        ax.set_yticklabels(names)\n",
        "        ax.grid(axis='x', alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.output_dir, 'training_time_comparison.png'),\n",
        "                   dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(\"âœ“ Training time comparison plot saved\")\n",
        "\n",
        "    def _plot_lr_vs_loss(self, results):\n",
        "        \"\"\"Plot learning rate vs final loss\"\"\"\n",
        "        lr_results = [r for r in results if 'learning_rate' in r['config_overrides']]\n",
        "\n",
        "        if not lr_results:\n",
        "            return\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "        lrs = [r['config_overrides']['learning_rate'] for r in lr_results]\n",
        "        losses = [r['statistics'].get('current_loss', 0) for r in lr_results]\n",
        "\n",
        "        ax.semilogx(lrs, losses, 'o-', markersize=10, linewidth=2)\n",
        "\n",
        "        for lr, loss in zip(lrs, losses):\n",
        "            ax.annotate(f'{loss:.4f}', (lr, loss),\n",
        "                       textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
        "\n",
        "        ax.set_xlabel('Learning Rate (log scale)', fontsize=12)\n",
        "        ax.set_ylabel('Final Loss', fontsize=12)\n",
        "        ax.set_title('Learning Rate vs Final Loss', fontsize=14, fontweight='bold')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.output_dir, 'lr_vs_loss.png'),\n",
        "                   dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(\"âœ“ Learning rate vs loss plot saved\")\n",
        "\n",
        "    def _plot_epoch_vs_loss(self, results):\n",
        "        \"\"\"Plot epochs vs final loss\"\"\"\n",
        "        epoch_results = [r for r in results if 'num_epochs' in r['config_overrides']]\n",
        "\n",
        "        if not epoch_results:\n",
        "            return\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "        epochs = [r['config_overrides']['num_epochs'] for r in epoch_results]\n",
        "        losses = [r['statistics'].get('current_loss', 0) for r in epoch_results]\n",
        "\n",
        "        ax.plot(epochs, losses, 'o-', markersize=12, linewidth=2, color='steelblue')\n",
        "\n",
        "        for ep, loss in zip(epochs, losses):\n",
        "            ax.annotate(f'{loss:.4f}', (ep, loss),\n",
        "                       textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
        "\n",
        "        ax.set_xlabel('Number of Epochs', fontsize=12)\n",
        "        ax.set_ylabel('Final Loss', fontsize=12)\n",
        "        ax.set_title('Training Epochs vs Final Loss', fontsize=14, fontweight='bold')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.output_dir, 'epoch_vs_loss.png'),\n",
        "                   dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(\"âœ“ Epochs vs loss plot saved\")\n",
        "\n",
        "    def _plot_ablation_heatmap(self, results):\n",
        "        \"\"\"Create comprehensive heatmap of all metrics\"\"\"\n",
        "        fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "        names = [r['experiment_name'] for r in results]\n",
        "        metrics = ['current_loss', 'avg_loss_last_100', 'min_loss',\n",
        "                  'max_loss', 'avg_grad_norm', 'num_alerts']\n",
        "\n",
        "        # Build matrix\n",
        "        matrix = []\n",
        "        for result in results:\n",
        "            stats = result['statistics']\n",
        "            row = [stats.get(m, 0) for m in metrics]\n",
        "            matrix.append(row)\n",
        "\n",
        "        matrix = np.array(matrix)\n",
        "\n",
        "        # Normalize each column for better visualization\n",
        "        from sklearn.preprocessing import MinMaxScaler\n",
        "        scaler = MinMaxScaler()\n",
        "        matrix_normalized = scaler.fit_transform(matrix)\n",
        "\n",
        "        # Create heatmap\n",
        "        sns.heatmap(matrix_normalized,\n",
        "                   xticklabels=metrics,\n",
        "                   yticklabels=names,\n",
        "                   annot=matrix,\n",
        "                   fmt='.4f',\n",
        "                   cmap='YlOrRd',\n",
        "                   cbar_kws={'label': 'Normalized Value'},\n",
        "                   ax=ax)\n",
        "\n",
        "        ax.set_title('Ablation Study: All Metrics Heatmap',\n",
        "                    fontsize=14, fontweight='bold', pad=20)\n",
        "        plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
        "        plt.setp(ax.get_yticklabels(), rotation=0)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.output_dir, 'ablation_heatmap.png'),\n",
        "                   dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(\"âœ“ Ablation heatmap saved\")\n",
        "\n",
        "# ============================================================================\n",
        "# INFERENCE AND EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "class InferenceEvaluator:\n",
        "    \"\"\"Evaluate trained ControlNet models\"\"\"\n",
        "\n",
        "    def __init__(self, model_path, base_model_name=\"runwayml/stable-diffusion-v1-5\"):\n",
        "        from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n",
        "\n",
        "        print(f\"Loading ControlNet from {model_path}...\")\n",
        "        controlnet = ControlNetModel.from_pretrained(model_path)\n",
        "\n",
        "        self.pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "            base_model_name,\n",
        "            controlnet=controlnet,\n",
        "            torch_dtype=torch.float16\n",
        "        )\n",
        "        self.pipe.to(\"cuda\")\n",
        "        print(\"âœ“ Pipeline loaded\")\n",
        "\n",
        "    def generate_samples(self, conditioning_images, prompts,\n",
        "                        output_dir, num_samples=4,\n",
        "                        conditioning_scales=[0.5, 0.75, 1.0, 1.5]):\n",
        "        \"\"\"Generate samples with different conditioning scales\"\"\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        results = []\n",
        "\n",
        "        for i, (cond_img, prompt) in enumerate(zip(conditioning_images, prompts)):\n",
        "            print(f\"\\nGenerating samples for image {i+1}/{len(conditioning_images)}\")\n",
        "\n",
        "            sample_results = {\n",
        "                'conditioning_image': cond_img,\n",
        "                'prompt': prompt,\n",
        "                'generated_images': {}\n",
        "            }\n",
        "\n",
        "            for scale in conditioning_scales:\n",
        "                print(f\"  Scale {scale}...\")\n",
        "\n",
        "                image = self.pipe(\n",
        "                    prompt,\n",
        "                    image=cond_img,\n",
        "                    num_inference_steps=50,\n",
        "                    controlnet_conditioning_scale=scale\n",
        "                ).images[0]\n",
        "\n",
        "                sample_results['generated_images'][scale] = image\n",
        "\n",
        "                # Save individual image\n",
        "                save_path = os.path.join(output_dir, f\"sample_{i}_scale_{scale}.png\")\n",
        "                image.save(save_path)\n",
        "\n",
        "            results.append(sample_results)\n",
        "\n",
        "            # Create comparison grid\n",
        "            self._create_comparison_grid(sample_results, output_dir, i)\n",
        "\n",
        "        print(f\"\\nâœ“ Generated {len(results)} sample sets\")\n",
        "        return results\n",
        "\n",
        "    def _create_comparison_grid(self, sample_result, output_dir, idx):\n",
        "        \"\"\"Create a grid comparing different conditioning scales\"\"\"\n",
        "        scales = sorted(sample_result['generated_images'].keys())\n",
        "\n",
        "        fig, axes = plt.subplots(1, len(scales) + 1, figsize=(4*(len(scales)+1), 4))\n",
        "\n",
        "        # Show conditioning image\n",
        "        axes[0].imshow(sample_result['conditioning_image'])\n",
        "        axes[0].set_title('Conditioning\\nImage', fontsize=10)\n",
        "        axes[0].axis('off')\n",
        "\n",
        "        # Show generated images\n",
        "        for i, scale in enumerate(scales, 1):\n",
        "            axes[i].imshow(sample_result['generated_images'][scale])\n",
        "            axes[i].set_title(f'Scale: {scale}', fontsize=10)\n",
        "            axes[i].axis('off')\n",
        "\n",
        "        plt.suptitle(f\"Prompt: {sample_result['prompt'][:50]}...\",\n",
        "                    fontsize=10, y=0.98)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        save_path = os.path.join(output_dir, f\"comparison_grid_{idx}.png\")\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     from controlnet_training import TrainingConfig\n",
        "\n",
        "#     # Setup base configuration\n",
        "#     base_config = TrainingConfig()\n",
        "#     base_config.num_epochs = 2  # Keep experiments shorter\n",
        "#     base_config.train_batch_size = 4\n",
        "\n",
        "#     # Path to dataset\n",
        "#     pkl_path = \"coco_pose_dataset.pkl\"\n",
        "\n",
        "#     # Create ablation runner\n",
        "#     runner = AblationRunner(base_config, pkl_path)\n",
        "\n",
        "#     # Get experiments to run (you can select subset)\n",
        "#     all_experiments = AblationSuite.get_all_experiments()\n",
        "\n",
        "#     # For quick testing, run a subset:\n",
        "#     experiments_to_run = [\n",
        "#         exp for exp in all_experiments\n",
        "#         if exp.name in ['lr_1e-05', 'lr_1e-04', 'epochs_1', 'epochs_3']\n",
        "#     ]\n",
        "\n",
        "#     # Run experiments\n",
        "#     runner.run_all_experiments(experiments_to_run)\n",
        "\n",
        "#     print(\"\\n\" + \"=\"*80)\n",
        "#     print(\"ABLATION STUDY COMPLETE\")\n",
        "#     print(\"=\"*80)\n",
        "#     print(f\"Results saved in: {runner.output_dir}\")"
      ],
      "metadata": {
        "id": "653DXv3Yzi7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Comprehensive Evaluation and Visualization for ControlNet\n",
        "Includes SSIM, LPIPS, pose alignment, and qualitative analysis\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from torchvision import transforms\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ============================================================================\n",
        "# METRICS\n",
        "# ============================================================================\n",
        "\n",
        "class MetricsCalculator:\n",
        "    \"\"\"Calculate various evaluation metrics\"\"\"\n",
        "\n",
        "    def __init__(self, device='cuda'):\n",
        "        self.device = device\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "\n",
        "        # Try to load LPIPS\n",
        "        try:\n",
        "            import lpips\n",
        "            self.lpips_fn = lpips.LPIPS(net='alex').to(device)\n",
        "            self.has_lpips = True\n",
        "            print(\"âœ“ LPIPS loaded\")\n",
        "        except ImportError:\n",
        "            print(\"âš ï¸  LPIPS not available. Install with: pip install lpips\")\n",
        "            self.has_lpips = False\n",
        "\n",
        "    def calculate_ssim(self, img1, img2):\n",
        "        \"\"\"Calculate SSIM between two images\"\"\"\n",
        "        # Convert to numpy if needed\n",
        "        if isinstance(img1, Image.Image):\n",
        "            img1 = np.array(img1)\n",
        "        if isinstance(img2, Image.Image):\n",
        "            img2 = np.array(img2)\n",
        "\n",
        "        # Ensure same size\n",
        "        if img1.shape != img2.shape:\n",
        "            img2 = np.array(Image.fromarray(img2).resize(\n",
        "                (img1.shape[1], img1.shape[0]), Image.BILINEAR\n",
        "            ))\n",
        "\n",
        "        # Calculate SSIM\n",
        "        if len(img1.shape) == 3:  # Color image\n",
        "            score = ssim(img1, img2, multichannel=True, channel_axis=2,\n",
        "                        data_range=255)\n",
        "        else:  # Grayscale\n",
        "            score = ssim(img1, img2, data_range=255)\n",
        "\n",
        "        return score\n",
        "\n",
        "    def calculate_lpips(self, img1, img2):\n",
        "        \"\"\"Calculate LPIPS perceptual distance\"\"\"\n",
        "        if not self.has_lpips:\n",
        "            return None\n",
        "\n",
        "        # Convert to tensors\n",
        "        if isinstance(img1, Image.Image):\n",
        "            img1 = self.to_tensor(img1).unsqueeze(0)\n",
        "        if isinstance(img2, Image.Image):\n",
        "            img2 = self.to_tensor(img2).unsqueeze(0)\n",
        "\n",
        "        # Ensure same size\n",
        "        if img1.shape != img2.shape:\n",
        "            img2 = F.interpolate(img2, size=img1.shape[2:],\n",
        "                                mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Normalize to [-1, 1]\n",
        "        img1 = img1 * 2 - 1\n",
        "        img2 = img2 * 2 - 1\n",
        "\n",
        "        # Calculate LPIPS\n",
        "        with torch.no_grad():\n",
        "            dist = self.lpips_fn(img1.to(self.device), img2.to(self.device))\n",
        "\n",
        "        return dist.item()\n",
        "\n",
        "    def calculate_mse(self, img1, img2):\n",
        "        \"\"\"Calculate MSE between two images\"\"\"\n",
        "        if isinstance(img1, Image.Image):\n",
        "            img1 = np.array(img1)\n",
        "        if isinstance(img2, Image.Image):\n",
        "            img2 = np.array(img2)\n",
        "\n",
        "        if img1.shape != img2.shape:\n",
        "            img2 = np.array(Image.fromarray(img2).resize(\n",
        "                (img1.shape[1], img1.shape[0]), Image.BILINEAR\n",
        "            ))\n",
        "\n",
        "        mse = np.mean((img1.astype(float) - img2.astype(float)) ** 2)\n",
        "        return mse\n",
        "\n",
        "    def calculate_psnr(self, img1, img2):\n",
        "        \"\"\"Calculate PSNR\"\"\"\n",
        "        mse = self.calculate_mse(img1, img2)\n",
        "        if mse == 0:\n",
        "            return float('inf')\n",
        "\n",
        "        max_pixel = 255.0\n",
        "        psnr = 20 * np.log10(max_pixel / np.sqrt(mse))\n",
        "        return psnr\n",
        "\n",
        "# ============================================================================\n",
        "# POSE CONSISTENCY EVALUATOR\n",
        "# ============================================================================\n",
        "\n",
        "class PoseConsistencyEvaluator:\n",
        "    \"\"\"Evaluate pose consistency between conditioning and generated images\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        try:\n",
        "            # Try to load a pose detector (e.g., MediaPipe or OpenPose)\n",
        "            import mediapipe as mp\n",
        "            self.mp_pose = mp.solutions.pose\n",
        "            self.pose = self.mp_pose.Pose(\n",
        "                static_image_mode=True,\n",
        "                model_complexity=2,\n",
        "                min_detection_confidence=0.5\n",
        "            )\n",
        "            self.has_pose_detector = True\n",
        "            print(\"âœ“ MediaPipe Pose loaded\")\n",
        "        except ImportError:\n",
        "            print(\"âš ï¸  MediaPipe not available. Pose consistency will be limited.\")\n",
        "            print(\"   Install with: pip install mediapipe\")\n",
        "            self.has_pose_detector = False\n",
        "\n",
        "    def extract_keypoints(self, image):\n",
        "        \"\"\"Extract pose keypoints from image\"\"\"\n",
        "        if not self.has_pose_detector:\n",
        "            return None\n",
        "\n",
        "        # Convert PIL to RGB numpy array\n",
        "        if isinstance(image, Image.Image):\n",
        "            image = np.array(image)\n",
        "\n",
        "        # Ensure RGB\n",
        "        if image.shape[2] == 4:  # RGBA\n",
        "            image = image[:, :, :3]\n",
        "\n",
        "        # Process image\n",
        "        results = self.pose.process(image)\n",
        "\n",
        "        if results.pose_landmarks:\n",
        "            # Extract keypoint coordinates\n",
        "            keypoints = []\n",
        "            for landmark in results.pose_landmarks.landmark:\n",
        "                keypoints.append([landmark.x, landmark.y, landmark.visibility])\n",
        "            return np.array(keypoints)\n",
        "\n",
        "        return None\n",
        "\n",
        "    def calculate_pose_similarity(self, img1, img2):\n",
        "        \"\"\"Calculate pose similarity between two images\"\"\"\n",
        "        if not self.has_pose_detector:\n",
        "            return None\n",
        "\n",
        "        kpts1 = self.extract_keypoints(img1)\n",
        "        kpts2 = self.extract_keypoints(img2)\n",
        "\n",
        "        if kpts1 is None or kpts2 is None:\n",
        "            return None\n",
        "\n",
        "        # Calculate Euclidean distance between keypoints\n",
        "        # Only consider visible keypoints (visibility > 0.5)\n",
        "        visible = (kpts1[:, 2] > 0.5) & (kpts2[:, 2] > 0.5)\n",
        "\n",
        "        if not visible.any():\n",
        "            return None\n",
        "\n",
        "        distance = np.linalg.norm(kpts1[visible, :2] - kpts2[visible, :2], axis=1)\n",
        "        avg_distance = np.mean(distance)\n",
        "\n",
        "        # Convert to similarity score (0-1, higher is better)\n",
        "        similarity = np.exp(-avg_distance * 10)  # Scale factor\n",
        "\n",
        "        return similarity\n",
        "\n",
        "# ============================================================================\n",
        "# COMPREHENSIVE EVALUATOR\n",
        "# ============================================================================\n",
        "\n",
        "class ComprehensiveEvaluator:\n",
        "    \"\"\"Complete evaluation pipeline\"\"\"\n",
        "\n",
        "    def __init__(self, output_dir=\"./evaluation_results\"):\n",
        "        self.output_dir = output_dir\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        self.metrics_calc = MetricsCalculator()\n",
        "        self.pose_evaluator = PoseConsistencyEvaluator()\n",
        "\n",
        "        self.results = []\n",
        "\n",
        "    def evaluate_sample(self, conditioning_img, generated_img, ground_truth_img,\n",
        "                       prompt, sample_id):\n",
        "        \"\"\"Evaluate a single generated sample\"\"\"\n",
        "        result = {\n",
        "            'sample_id': sample_id,\n",
        "            'prompt': prompt,\n",
        "            'metrics': {}\n",
        "        }\n",
        "\n",
        "        # SSIM\n",
        "        ssim_score = self.metrics_calc.calculate_ssim(generated_img, ground_truth_img)\n",
        "        result['metrics']['ssim'] = ssim_score\n",
        "\n",
        "        # LPIPS\n",
        "        lpips_score = self.metrics_calc.calculate_lpips(generated_img, ground_truth_img)\n",
        "        if lpips_score is not None:\n",
        "            result['metrics']['lpips'] = lpips_score\n",
        "\n",
        "        # MSE and PSNR\n",
        "        mse = self.metrics_calc.calculate_mse(generated_img, ground_truth_img)\n",
        "        psnr = self.metrics_calc.calculate_psnr(generated_img, ground_truth_img)\n",
        "        result['metrics']['mse'] = mse\n",
        "        result['metrics']['psnr'] = psnr\n",
        "\n",
        "        # Pose consistency\n",
        "        pose_sim = self.pose_evaluator.calculate_pose_similarity(\n",
        "            conditioning_img, generated_img\n",
        "        )\n",
        "        if pose_sim is not None:\n",
        "            result['metrics']['pose_similarity'] = pose_sim\n",
        "\n",
        "        self.results.append(result)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def evaluate_batch(self, conditioning_images, generated_images,\n",
        "                      ground_truth_images, prompts):\n",
        "        \"\"\"Evaluate a batch of samples\"\"\"\n",
        "        print(f\"Evaluating {len(generated_images)} samples...\")\n",
        "\n",
        "        for i in tqdm(range(len(generated_images))):\n",
        "            self.evaluate_sample(\n",
        "                conditioning_images[i],\n",
        "                generated_images[i],\n",
        "                ground_truth_images[i],\n",
        "                prompts[i],\n",
        "                sample_id=i\n",
        "            )\n",
        "\n",
        "        print(\"âœ“ Evaluation complete\")\n",
        "\n",
        "        # Calculate aggregate statistics\n",
        "        self.calculate_aggregate_stats()\n",
        "\n",
        "        return self.results\n",
        "\n",
        "    def calculate_aggregate_stats(self):\n",
        "        \"\"\"Calculate aggregate statistics across all samples\"\"\"\n",
        "        if not self.results:\n",
        "            return {}\n",
        "\n",
        "        metrics_names = list(self.results[0]['metrics'].keys())\n",
        "\n",
        "        self.aggregate_stats = {}\n",
        "        for metric in metrics_names:\n",
        "            values = [r['metrics'][metric] for r in self.results\n",
        "                     if metric in r['metrics'] and r['metrics'][metric] is not None]\n",
        "\n",
        "            if values:\n",
        "                self.aggregate_stats[metric] = {\n",
        "                    'mean': np.mean(values),\n",
        "                    'std': np.std(values),\n",
        "                    'min': np.min(values),\n",
        "                    'max': np.max(values),\n",
        "                    'median': np.median(values)\n",
        "                }\n",
        "\n",
        "        return self.aggregate_stats\n",
        "\n",
        "    def save_results(self):\n",
        "        \"\"\"Save evaluation results to JSON\"\"\"\n",
        "        results_data = {\n",
        "            'individual_results': self.results,\n",
        "            'aggregate_stats': self.aggregate_stats\n",
        "        }\n",
        "\n",
        "        save_path = os.path.join(self.output_dir, 'evaluation_results.json')\n",
        "        with open(save_path, 'w') as f:\n",
        "            json.dump(results_data, f, indent=2)\n",
        "\n",
        "        print(f\"âœ“ Results saved to {save_path}\")\n",
        "\n",
        "    def visualize_results(self):\n",
        "        \"\"\"Create comprehensive visualizations\"\"\"\n",
        "        if not self.results:\n",
        "            print(\"No results to visualize\")\n",
        "            return\n",
        "\n",
        "        # 1. Metrics distribution\n",
        "        self._plot_metrics_distribution()\n",
        "\n",
        "        # 2. Metrics correlation\n",
        "        self._plot_metrics_correlation()\n",
        "\n",
        "        # 3. Per-sample metrics\n",
        "        self._plot_per_sample_metrics()\n",
        "\n",
        "        # 4. Best and worst samples\n",
        "        self._create_qualitative_comparison()\n",
        "\n",
        "    def _plot_metrics_distribution(self):\n",
        "        \"\"\"Plot distribution of each metric\"\"\"\n",
        "        metrics_names = list(self.aggregate_stats.keys())\n",
        "\n",
        "        n_metrics = len(metrics_names)\n",
        "        fig, axes = plt.subplots(2, (n_metrics + 1) // 2, figsize=(15, 8))\n",
        "        axes = axes.flatten()\n",
        "\n",
        "        for i, metric in enumerate(metrics_names):\n",
        "            values = [r['metrics'][metric] for r in self.results\n",
        "                     if metric in r['metrics'] and r['metrics'][metric] is not None]\n",
        "\n",
        "            axes[i].hist(values, bins=20, edgecolor='black', alpha=0.7)\n",
        "            axes[i].axvline(self.aggregate_stats[metric]['mean'],\n",
        "                           color='red', linestyle='--', linewidth=2,\n",
        "                           label=f\"Mean: {self.aggregate_stats[metric]['mean']:.3f}\")\n",
        "            axes[i].set_xlabel(metric.upper(), fontsize=10)\n",
        "            axes[i].set_ylabel('Frequency', fontsize=10)\n",
        "            axes[i].set_title(f'{metric.upper()} Distribution', fontsize=11, fontweight='bold')\n",
        "            axes[i].legend()\n",
        "            axes[i].grid(alpha=0.3)\n",
        "\n",
        "        # Hide extra subplots\n",
        "        for i in range(n_metrics, len(axes)):\n",
        "            axes[i].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.output_dir, 'metrics_distribution.png'),\n",
        "                   dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(\"âœ“ Metrics distribution plot saved\")\n",
        "\n",
        "    def _plot_metrics_correlation(self):\n",
        "        \"\"\"Plot correlation between metrics\"\"\"\n",
        "        metrics_names = list(self.aggregate_stats.keys())\n",
        "\n",
        "        # Build correlation matrix\n",
        "        n_metrics = len(metrics_names)\n",
        "        corr_matrix = np.zeros((n_metrics, n_metrics))\n",
        "\n",
        "        for i, metric1 in enumerate(metrics_names):\n",
        "            for j, metric2 in enumerate(metrics_names):\n",
        "                values1 = [r['metrics'][metric1] for r in self.results\n",
        "                          if metric1 in r['metrics'] and r['metrics'][metric1] is not None]\n",
        "                values2 = [r['metrics'][metric2] for r in self.results\n",
        "                          if metric2 in r['metrics'] and r['metrics'][metric2] is not None]\n",
        "\n",
        "                if len(values1) == len(values2) and len(values1) > 1:\n",
        "                    corr_matrix[i, j] = np.corrcoef(values1, values2)[0, 1]\n",
        "\n",
        "        # Plot heatmap\n",
        "        fig, ax = plt.subplots(figsize=(10, 8))\n",
        "        sns.heatmap(corr_matrix,\n",
        "                   xticklabels=[m.upper() for m in metrics_names],\n",
        "                   yticklabels=[m.upper() for m in metrics_names],\n",
        "                   annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
        "                   vmin=-1, vmax=1, ax=ax,\n",
        "                   cbar_kws={'label': 'Correlation'})\n",
        "\n",
        "        ax.set_title('Metrics Correlation Heatmap', fontsize=14, fontweight='bold', pad=20)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.output_dir, 'metrics_correlation.png'),\n",
        "                   dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(\"âœ“ Metrics correlation plot saved\")\n",
        "\n",
        "    def _plot_per_sample_metrics(self):\n",
        "        \"\"\"Plot metrics for each sample\"\"\"\n",
        "        metrics_names = list(self.aggregate_stats.keys())\n",
        "        n_samples = len(self.results)\n",
        "\n",
        "        fig, axes = plt.subplots(len(metrics_names), 1,\n",
        "                                figsize=(12, 3*len(metrics_names)))\n",
        "\n",
        "        if len(metrics_names) == 1:\n",
        "            axes = [axes]\n",
        "\n",
        "        for i, metric in enumerate(metrics_names):\n",
        "            values = [r['metrics'].get(metric, 0) for r in self.results]\n",
        "            sample_ids = list(range(n_samples))\n",
        "\n",
        "            axes[i].plot(sample_ids, values, marker='o', linewidth=1, markersize=4)\n",
        "            axes[i].axhline(self.aggregate_stats[metric]['mean'],\n",
        "                          color='red', linestyle='--', linewidth=2,\n",
        "                          label=f\"Mean: {self.aggregate_stats[metric]['mean']:.3f}\")\n",
        "            axes[i].set_xlabel('Sample ID', fontsize=10)\n",
        "            axes[i].set_ylabel(metric.upper(), fontsize=10)\n",
        "            axes[i].set_title(f'{metric.upper()} per Sample',\n",
        "                            fontsize=11, fontweight='bold')\n",
        "            axes[i].legend()\n",
        "            axes[i].grid(alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.output_dir, 'per_sample_metrics.png'),\n",
        "                   dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(\"âœ“ Per-sample metrics plot saved\")\n",
        "\n",
        "    def _create_qualitative_comparison(self):\n",
        "        \"\"\"Create visualization of best and worst samples\"\"\"\n",
        "        # Find best and worst samples based on SSIM\n",
        "        if 'ssim' not in self.aggregate_stats:\n",
        "            print(\"âš ï¸  SSIM not available for qualitative comparison\")\n",
        "            return\n",
        "\n",
        "        ssim_scores = [(i, r['metrics']['ssim']) for i, r in enumerate(self.results)\n",
        "                      if 'ssim' in r['metrics']]\n",
        "        ssim_scores.sort(key=lambda x: x[1])\n",
        "\n",
        "        # Get 3 worst and 3 best\n",
        "        worst_ids = [idx for idx, _ in ssim_scores[:3]]\n",
        "        best_ids = [idx for idx, _ in ssim_scores[-3:]]\n",
        "\n",
        "        print(f\"âœ“ Best samples (SSIM): {[ssim_scores[i][1] for i in range(-3, 0)]}\")\n",
        "        print(f\"âœ“ Worst samples (SSIM): {[ssim_scores[i][1] for i in range(3)]}\")\n",
        "\n",
        "        # Note: This would require access to actual images\n",
        "        # You would need to pass images to the evaluator for this to work\n",
        "        print(\"âœ“ Qualitative comparison identified (visualization requires images)\")\n",
        "\n",
        "# ============================================================================\n",
        "# FAILURE ANALYSIS\n",
        "# ============================================================================\n",
        "\n",
        "class FailureAnalyzer:\n",
        "    \"\"\"Analyze failure cases\"\"\"\n",
        "\n",
        "    def __init__(self, results, threshold_ssim=0.5):\n",
        "        self.results = results\n",
        "        self.threshold_ssim = threshold_ssim\n",
        "        self.failure_cases = []\n",
        "\n",
        "    def identify_failures(self):\n",
        "        \"\"\"Identify failure cases based on metrics\"\"\"\n",
        "        for result in self.results:\n",
        "            is_failure = False\n",
        "            failure_reasons = []\n",
        "\n",
        "            # Check SSIM\n",
        "            if 'ssim' in result['metrics']:\n",
        "                if result['metrics']['ssim'] < self.threshold_ssim:\n",
        "                    is_failure = True\n",
        "                    failure_reasons.append(\n",
        "                        f\"Low SSIM: {result['metrics']['ssim']:.3f}\"\n",
        "                    )\n",
        "\n",
        "            # Check pose similarity\n",
        "            if 'pose_similarity' in result['metrics']:\n",
        "                if result['metrics']['pose_similarity'] < 0.5:\n",
        "                    is_failure = True\n",
        "                    failure_reasons.append(\n",
        "                        f\"Low pose similarity: {result['metrics']['pose_similarity']:.3f}\"\n",
        "                    )\n",
        "\n",
        "            if is_failure:\n",
        "                self.failure_cases.append({\n",
        "                    'sample_id': result['sample_id'],\n",
        "                    'prompt': result['prompt'],\n",
        "                    'metrics': result['metrics'],\n",
        "                    'reasons': failure_reasons\n",
        "                })\n",
        "\n",
        "        print(f\"Identified {len(self.failure_cases)} failure cases \"\n",
        "              f\"({len(self.failure_cases)/len(self.results)*100:.1f}%)\")\n",
        "\n",
        "        return self.failure_cases\n",
        "\n",
        "    def categorize_failures(self):\n",
        "        \"\"\"Categorize failure types\"\"\"\n",
        "        categories = {\n",
        "            'low_ssim': [],\n",
        "            'low_pose_similarity': [],\n",
        "            'high_lpips': []\n",
        "        }\n",
        "\n",
        "        for failure in self.failure_cases:\n",
        "            if any('SSIM' in reason for reason in failure['reasons']):\n",
        "                categories['low_ssim'].append(failure)\n",
        "            if any('pose' in reason for reason in failure['reasons']):\n",
        "                categories['low_pose_similarity'].append(failure)\n",
        "            if 'lpips' in failure['metrics'] and failure['metrics']['lpips'] > 0.5:\n",
        "                categories['high_lpips'].append(failure)\n",
        "\n",
        "        print(\"\\nFailure Categories:\")\n",
        "        for category, cases in categories.items():\n",
        "            print(f\"  {category}: {len(cases)} cases\")\n",
        "\n",
        "        return categories\n",
        "\n",
        "    def generate_failure_report(self, output_path):\n",
        "        \"\"\"Generate detailed failure analysis report\"\"\"\n",
        "        report = {\n",
        "            'total_samples': len(self.results),\n",
        "            'num_failures': len(self.failure_cases),\n",
        "            'failure_rate': len(self.failure_cases) / len(self.results),\n",
        "            'failure_cases': self.failure_cases,\n",
        "            'categories': self.categorize_failures()\n",
        "        }\n",
        "\n",
        "        with open(output_path, 'w') as f:\n",
        "            json.dump(report, f, indent=2)\n",
        "\n",
        "        print(f\"âœ“ Failure report saved to {output_path}\")\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN USAGE EXAMPLE\n",
        "# ============================================================================\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     # Example usage\n",
        "#     evaluator = ComprehensiveEvaluator()\n",
        "\n",
        "#     # Load your generated samples, conditioning images, and ground truth\n",
        "#     # conditioning_images = [...]\n",
        "#     # generated_images = [...]\n",
        "#     # ground_truth_images = [...]\n",
        "#     # prompts = [...]\n",
        "\n",
        "#     # Run evaluation\n",
        "#     # results = evaluator.evaluate_batch(\n",
        "#     #     conditioning_images,\n",
        "#     #     generated_images,\n",
        "#     #     ground_truth_images,\n",
        "#     #     prompts\n",
        "#     # )\n",
        "\n",
        "#     # Visualize\n",
        "#     # evaluator.visualize_results()\n",
        "#     # evaluator.save_results()\n",
        "\n",
        "#     # Failure analysis\n",
        "#     # analyzer = FailureAnalyzer(evaluator.results)\n",
        "#     # analyzer.identify_failures()\n",
        "#     # analyzer.generate_failure_report('failure_report.json')\n",
        "\n",
        "#     print(\"âœ“ Evaluation framework ready\")"
      ],
      "metadata": {
        "id": "GOmKzZoizhEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KJDOe0_0yROl",
        "outputId": "8d61488e-ed73-4a0a-dd82-8c97e50b48e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "CONTROLNET PROJECT: COMPLETE PIPELINE\n",
            "Pose-to-Person Image Generation\n",
            "================================================================================\n",
            "\n",
            "PHASE 0: DATASET PREPARATION\n",
            "--------------------------------------------------------------------------------\n",
            "Downloading dataset from Google Drive...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1wtUgBGHfUaJ_mxQL02t6WGfDk2QNN7uI\n",
            "From (redirected): https://drive.google.com/uc?id=1wtUgBGHfUaJ_mxQL02t6WGfDk2QNN7uI&confirm=t&uuid=05812140-04f9-4e52-a5c3-ee129363438e\n",
            "To: /content/coco_pose_dataset.pkl\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 248M/248M [00:04<00:00, 54.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Dataset downloaded to ./coco_pose_dataset.pkl\n",
            "\n",
            "Loading dataset from ./coco_pose_dataset.pkl...\n",
            "âœ“ Loaded 203 samples\n",
            "\n",
            "Sample structure:\n",
            "  - image_id: 537548\n",
            "  - img shape: (480, 640, 3)\n",
            "  - skeleton shape: (480, 640, 3)\n",
            "  - text_prompt: A  soldier riding a red motorcycle down a busy str...\n",
            "âœ“ Sample visualization saved to ./dataset_inspection/sample_visualization.png\n",
            "\n",
            "================================================================================\n",
            "PHASE 1: MAIN TRAINING\n",
            "================================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'controlnet_training'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4185442438.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[0;31m# Run pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-4185442438.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;31m# Phase 1: Main Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m     \u001b[0mtraining_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Phase 2: Ablation Studies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4185442438.py\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m80\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mcontrolnet_training\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainingConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_controlnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;31m# Setup training config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'controlnet_training'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "\"\"\"\n",
        "Complete End-to-End ControlNet Pipeline\n",
        "Train, Evaluate, and Visualize with One Script\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "import pickle\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import gdown  # For downloading from Google Drive\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "class PipelineConfig:\n",
        "    \"\"\"Master configuration for entire pipeline\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Data\n",
        "        self.google_drive_url = \"https://drive.google.com/file/d/1wtUgBGHfUaJ_mxQL02t6WGfDk2QNN7uI/view?usp=sharing\"\n",
        "        self.data_path = \"./coco_pose_dataset.pkl\"\n",
        "\n",
        "        # Training\n",
        "        self.pretrained_model = \"runwayml/stable-diffusion-v1-5\"\n",
        "        self.num_epochs = 3\n",
        "        self.train_batch_size = 4\n",
        "        self.learning_rate = 1e-5\n",
        "        self.resolution = 512\n",
        "\n",
        "        # Ablation\n",
        "        self.run_ablations = True\n",
        "        self.ablation_experiments = [\n",
        "            'lr_1e-05', 'lr_1e-04', 'lr_2e-06',\n",
        "            'epochs_1', 'epochs_3',\n",
        "            'scale_0.5', 'scale_1.0', 'scale_1.5'\n",
        "        ]\n",
        "\n",
        "        # Evaluation\n",
        "        self.num_eval_samples = 50\n",
        "        self.generate_qualitative = True\n",
        "\n",
        "        # Output\n",
        "        self.output_dir = \"./controlnet_project_output\"\n",
        "        self.save_checkpoints = True\n",
        "\n",
        "# ============================================================================\n",
        "# DATA LOADER\n",
        "# ============================================================================\n",
        "\n",
        "def download_dataset(url, output_path):\n",
        "    \"\"\"Download dataset from Google Drive\"\"\"\n",
        "    if os.path.exists(output_path):\n",
        "        print(f\"âœ“ Dataset already exists at {output_path}\")\n",
        "        return True\n",
        "\n",
        "    print(f\"Downloading dataset from Google Drive...\")\n",
        "    try:\n",
        "        gdown.download(url, output_path, quiet=False, fuzzy=True)\n",
        "        print(f\"âœ“ Dataset downloaded to {output_path}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Failed to download dataset: {e}\")\n",
        "        print(\"\\nAlternative: Manually download from:\")\n",
        "        print(\"https://drive.google.com/drive/folders/1uOpYTO7MJvOcGhVHUsXLjzxbpIEsJzn-\")\n",
        "        return False\n",
        "\n",
        "def load_and_inspect_dataset(pkl_path):\n",
        "    \"\"\"Load dataset and print statistics\"\"\"\n",
        "    print(f\"\\nLoading dataset from {pkl_path}...\")\n",
        "\n",
        "    with open(pkl_path, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "\n",
        "    print(f\"âœ“ Loaded {len(data)} samples\")\n",
        "\n",
        "    # Print sample info\n",
        "    sample = data[0]\n",
        "    print(f\"\\nSample structure:\")\n",
        "    print(f\"  - image_id: {sample['image_id']}\")\n",
        "    print(f\"  - img shape: {sample['img'].shape}\")\n",
        "    print(f\"  - skeleton shape: {sample['skeleton'].shape}\")\n",
        "    print(f\"  - text_prompt: {sample['text_prompt'][:50]}...\")\n",
        "\n",
        "    # Visualize a sample\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "    axes[0].imshow(sample['img'])\n",
        "    axes[0].set_title('Original Image')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    axes[1].imshow(sample['skeleton'])\n",
        "    axes[1].set_title('Pose Skeleton (Control Signal)')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    plt.suptitle(f\"Prompt: {sample['text_prompt']}\", fontsize=10)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    os.makedirs('./dataset_inspection', exist_ok=True)\n",
        "    plt.savefig('./dataset_inspection/sample_visualization.png', dpi=150)\n",
        "    plt.close()\n",
        "    print(\"âœ“ Sample visualization saved to ./dataset_inspection/sample_visualization.png\")\n",
        "\n",
        "    return data\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING PHASE\n",
        "# ============================================================================\n",
        "\n",
        "def run_training(config):\n",
        "    \"\"\"Run main training\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PHASE 1: MAIN TRAINING\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    # from controlnet_training import TrainingConfig, train_controlnet\n",
        "\n",
        "    # Setup training config\n",
        "    train_config = TrainingConfig()\n",
        "    train_config.num_epochs = config.num_epochs\n",
        "    train_config.train_batch_size = config.train_batch_size\n",
        "    train_config.learning_rate = config.learning_rate\n",
        "    train_config.resolution = config.resolution\n",
        "    train_config.output_dir = os.path.join(config.output_dir, \"main_training\")\n",
        "\n",
        "    # Run training\n",
        "    monitor = train_controlnet(train_config, config.data_path)\n",
        "\n",
        "    print(\"\\nâœ“ Main training complete\")\n",
        "    print(f\"âœ“ Model saved to {train_config.output_dir}/final_model\")\n",
        "\n",
        "    return train_config.output_dir\n",
        "\n",
        "# ============================================================================\n",
        "# ABLATION PHASE\n",
        "# ============================================================================\n",
        "\n",
        "def run_ablations(config):\n",
        "    \"\"\"Run ablation studies\"\"\"\n",
        "    if not config.run_ablations:\n",
        "        print(\"\\nâŠ˜ Skipping ablation studies (disabled in config)\")\n",
        "        return None\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PHASE 2: ABLATION STUDIES\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    # from controlnet_training import TrainingConfig\n",
        "    # from ablation_study import AblationRunner, AblationSuite\n",
        "\n",
        "    # Setup base config\n",
        "    base_config = TrainingConfig()\n",
        "    base_config.num_epochs = 2  # Shorter for ablations\n",
        "    base_config.train_batch_size = config.train_batch_size\n",
        "    base_config.resolution = config.resolution\n",
        "\n",
        "    # Create runner\n",
        "    ablation_output = os.path.join(config.output_dir, \"ablations\")\n",
        "    runner = AblationRunner(base_config, config.data_path, ablation_output)\n",
        "\n",
        "    # Get experiments\n",
        "    all_experiments = AblationSuite.get_all_experiments()\n",
        "\n",
        "    # Filter to selected experiments\n",
        "    if config.ablation_experiments:\n",
        "        experiments = [\n",
        "            exp for exp in all_experiments\n",
        "            if exp.name in config.ablation_experiments\n",
        "        ]\n",
        "    else:\n",
        "        experiments = all_experiments[:5]  # Run first 5 if not specified\n",
        "\n",
        "    print(f\"Running {len(experiments)} ablation experiments...\")\n",
        "\n",
        "    # Run ablations\n",
        "    runner.run_all_experiments(experiments)\n",
        "\n",
        "    print(\"\\nâœ“ Ablation studies complete\")\n",
        "    print(f\"âœ“ Results saved to {ablation_output}\")\n",
        "\n",
        "    return ablation_output\n",
        "\n",
        "# ============================================================================\n",
        "# EVALUATION PHASE\n",
        "# ============================================================================\n",
        "\n",
        "def run_evaluation(config, model_path):\n",
        "    \"\"\"Run comprehensive evaluation\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PHASE 3: EVALUATION\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    # from evaluation_viz import ComprehensiveEvaluator, FailureAnalyzer\n",
        "    from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n",
        "\n",
        "    # Load dataset for evaluation\n",
        "    with open(config.data_path, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "\n",
        "    # Take subset for evaluation\n",
        "    eval_data = data[-config.num_eval_samples:]\n",
        "\n",
        "    print(f\"Evaluating on {len(eval_data)} samples...\")\n",
        "\n",
        "    # Load model\n",
        "    try:\n",
        "        controlnet = ControlNetModel.from_pretrained(\n",
        "            os.path.join(model_path, \"final_model\")\n",
        "        )\n",
        "\n",
        "        pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "            config.pretrained_model,\n",
        "            controlnet=controlnet,\n",
        "            torch_dtype=torch.float16\n",
        "        )\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            pipe = pipe.to(\"cuda\")\n",
        "            print(\"âœ“ Using CUDA\")\n",
        "        else:\n",
        "            print(\"âš ï¸  CUDA not available, using CPU (slow)\")\n",
        "\n",
        "        print(\"âœ“ Pipeline loaded\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Failed to load model: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Generate samples\n",
        "    eval_output = os.path.join(config.output_dir, \"evaluation\")\n",
        "    os.makedirs(eval_output, exist_ok=True)\n",
        "\n",
        "    conditioning_images = []\n",
        "    generated_images = []\n",
        "    ground_truth_images = []\n",
        "    prompts = []\n",
        "\n",
        "    print(\"\\nGenerating samples for evaluation...\")\n",
        "    for i, sample in enumerate(eval_data[:10]):  # Generate 10 for detailed analysis\n",
        "        print(f\"  Sample {i+1}/10...\")\n",
        "\n",
        "        # Prepare conditioning image\n",
        "        cond_img = Image.fromarray(sample['skeleton']).convert('RGB')\n",
        "        cond_img = cond_img.resize((config.resolution, config.resolution))\n",
        "\n",
        "        # Generate\n",
        "        with torch.no_grad():\n",
        "            output = pipe(\n",
        "                sample['text_prompt'],\n",
        "                image=cond_img,\n",
        "                num_inference_steps=50,\n",
        "                guidance_scale=7.5\n",
        "            )\n",
        "\n",
        "        gen_img = output.images[0]\n",
        "\n",
        "        # Ground truth\n",
        "        gt_img = Image.fromarray(sample['img']).convert('RGB')\n",
        "        gt_img = gt_img.resize((config.resolution, config.resolution))\n",
        "\n",
        "        conditioning_images.append(cond_img)\n",
        "        generated_images.append(gen_img)\n",
        "        ground_truth_images.append(gt_img)\n",
        "        prompts.append(sample['text_prompt'])\n",
        "\n",
        "        # Save individual result\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "        axes[0].imshow(cond_img)\n",
        "        axes[0].set_title('Conditioning (Pose)')\n",
        "        axes[0].axis('off')\n",
        "\n",
        "        axes[1].imshow(gen_img)\n",
        "        axes[1].set_title('Generated')\n",
        "        axes[1].axis('off')\n",
        "\n",
        "        axes[2].imshow(gt_img)\n",
        "        axes[2].set_title('Ground Truth')\n",
        "        axes[2].axis('off')\n",
        "\n",
        "        plt.suptitle(f\"Prompt: {sample['text_prompt'][:60]}...\", fontsize=9)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(eval_output, f'sample_{i:03d}.png'), dpi=150)\n",
        "        plt.close()\n",
        "\n",
        "    print(\"âœ“ Samples generated\")\n",
        "\n",
        "    # Run metrics evaluation\n",
        "    evaluator = ComprehensiveEvaluator(eval_output)\n",
        "    results = evaluator.evaluate_batch(\n",
        "        conditioning_images,\n",
        "        generated_images,\n",
        "        ground_truth_images,\n",
        "        prompts\n",
        "    )\n",
        "\n",
        "    # Visualize results\n",
        "    evaluator.visualize_results()\n",
        "    evaluator.save_results()\n",
        "\n",
        "    # Failure analysis\n",
        "    analyzer = FailureAnalyzer(evaluator.results)\n",
        "    analyzer.identify_failures()\n",
        "    analyzer.generate_failure_report(os.path.join(eval_output, 'failure_report.json'))\n",
        "\n",
        "    print(\"\\nâœ“ Evaluation complete\")\n",
        "    print(f\"âœ“ Results saved to {eval_output}\")\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"EVALUATION SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "    for metric, stats in evaluator.aggregate_stats.items():\n",
        "        print(f\"\\n{metric.upper()}:\")\n",
        "        print(f\"  Mean: {stats['mean']:.4f}\")\n",
        "        print(f\"  Std:  {stats['std']:.4f}\")\n",
        "        print(f\"  Min:  {stats['min']:.4f}\")\n",
        "        print(f\"  Max:  {stats['max']:.4f}\")\n",
        "\n",
        "    return eval_output\n",
        "\n",
        "# ============================================================================\n",
        "# REPORT GENERATION\n",
        "# ============================================================================\n",
        "\n",
        "def generate_final_report(config, training_dir, ablation_dir, eval_dir):\n",
        "    \"\"\"Generate final comprehensive report\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"GENERATING FINAL REPORT\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    report_dir = os.path.join(config.output_dir, \"final_report\")\n",
        "    os.makedirs(report_dir, exist_ok=True)\n",
        "\n",
        "    # Collect all results\n",
        "    report_content = {\n",
        "        'project': 'ControlNet for Pose-to-Person Generation',\n",
        "        'configuration': {\n",
        "            'model': config.pretrained_model,\n",
        "            'epochs': config.num_epochs,\n",
        "            'batch_size': config.train_batch_size,\n",
        "            'learning_rate': config.learning_rate,\n",
        "            'resolution': config.resolution\n",
        "        },\n",
        "        'training_directory': training_dir,\n",
        "        'ablation_directory': ablation_dir,\n",
        "        'evaluation_directory': eval_dir\n",
        "    }\n",
        "\n",
        "    # Save report\n",
        "    import json\n",
        "    report_path = os.path.join(report_dir, 'project_report.json')\n",
        "    with open(report_path, 'w') as f:\n",
        "        json.dump(report_content, f, indent=2)\n",
        "\n",
        "    # Create summary markdown\n",
        "    md_content = f\"\"\"# ControlNet Project: Pose-to-Person Generation\n",
        "\n",
        "## Project Overview\n",
        "This project implements a ControlNet for generating person images from pose skeletons.\n",
        "\n",
        "## Configuration\n",
        "- **Base Model**: {config.pretrained_model}\n",
        "- **Training Epochs**: {config.num_epochs}\n",
        "- **Batch Size**: {config.train_batch_size}\n",
        "- **Learning Rate**: {config.learning_rate}\n",
        "- **Resolution**: {config.resolution}\n",
        "\n",
        "## Results Structure\n",
        "\n",
        "### 1. Training Results\n",
        "Location: `{training_dir}`\n",
        "- Final trained model\n",
        "- Training curves\n",
        "- Training logs and reports\n",
        "\n",
        "### 2. Ablation Studies\n",
        "Location: `{ablation_dir}`\n",
        "- Multiple experiment configurations\n",
        "- Comparative analysis\n",
        "- Ablation visualizations\n",
        "\n",
        "### 3. Evaluation Results\n",
        "Location: `{eval_dir}`\n",
        "- Quantitative metrics (SSIM, LPIPS, PSNR)\n",
        "- Qualitative samples\n",
        "- Failure analysis\n",
        "\n",
        "## Key Files\n",
        "- `training_curves.png` - Training loss over time\n",
        "- `evaluation_results.json` - Detailed metrics\n",
        "- `ablation_results.json` - Ablation study results\n",
        "- `failure_report.json` - Analysis of failure cases\n",
        "\n",
        "## Next Steps\n",
        "1. Review evaluation metrics in `evaluation_results.json`\n",
        "2. Examine ablation comparisons in `ablation_results.json`\n",
        "3. Analyze failure cases for improvements\n",
        "4. Fine-tune based on insights\n",
        "\n",
        "---\n",
        "Generated: {config.output_dir}\n",
        "\"\"\"\n",
        "\n",
        "    md_path = os.path.join(report_dir, 'README.md')\n",
        "    with open(md_path, 'w') as f:\n",
        "        f.write(md_content)\n",
        "\n",
        "    print(f\"âœ“ Final report generated at {report_dir}\")\n",
        "    print(f\"âœ“ Summary: {md_path}\")\n",
        "\n",
        "    return report_dir\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution pipeline\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"CONTROLNET PROJECT: COMPLETE PIPELINE\")\n",
        "    print(\"Pose-to-Person Image Generation\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    # Initialize configuration\n",
        "    config = PipelineConfig()\n",
        "    os.makedirs(config.output_dir, exist_ok=True)\n",
        "\n",
        "    # Phase 0: Download and inspect dataset\n",
        "    print(\"PHASE 0: DATASET PREPARATION\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    if not os.path.exists(config.data_path):\n",
        "        success = download_dataset(config.google_drive_url, config.data_path)\n",
        "        if not success:\n",
        "            print(\"\\nâŒ Cannot proceed without dataset\")\n",
        "            return\n",
        "\n",
        "    data = load_and_inspect_dataset(config.data_path)\n",
        "\n",
        "    # Phase 1: Main Training\n",
        "    training_dir = run_training(config)\n",
        "\n",
        "    # Phase 2: Ablation Studies\n",
        "    ablation_dir = run_ablations(config)\n",
        "\n",
        "    # Phase 3: Evaluation\n",
        "    eval_dir = run_evaluation(config, training_dir)\n",
        "\n",
        "    # Phase 4: Generate Final Report\n",
        "    if eval_dir:\n",
        "        report_dir = generate_final_report(\n",
        "            config, training_dir, ablation_dir, eval_dir\n",
        "        )\n",
        "\n",
        "    # Final summary\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PIPELINE COMPLETE!\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\nâœ“ All outputs saved to: {config.output_dir}\")\n",
        "    print(\"\\nDirectory structure:\")\n",
        "    print(f\"  â”œâ”€â”€ main_training/       (trained model and logs)\")\n",
        "    print(f\"  â”œâ”€â”€ ablations/          (ablation study results)\")\n",
        "    print(f\"  â”œâ”€â”€ evaluation/         (metrics and samples)\")\n",
        "    print(f\"  â””â”€â”€ final_report/       (comprehensive summary)\")\n",
        "    print(\"\\nğŸ‰ Project complete! Review the results and generate your report.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Parse command line arguments (optional)\n",
        "    # parser = argparse.ArgumentParser(description='ControlNet Training Pipeline')\n",
        "    # parser.add_argument('--skip-ablations', action='store_true',\n",
        "    #                    help='Skip ablation studies')\n",
        "    # parser.add_argument('--epochs', type=int, default=3,\n",
        "    #                    help='Number of training epochs')\n",
        "    # parser.add_argument('--batch-size', type=int, default=4,\n",
        "    #                    help='Training batch size')\n",
        "\n",
        "    # args = parser.parse_args()\n",
        "\n",
        "    # Run pipeline\n",
        "    main()"
      ]
    }
  ]
}