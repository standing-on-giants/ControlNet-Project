# -*- coding: utf-8 -*-
"""Ablation_Study.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y15nd2MB2aKVw0VzBndgWys7e8CxTZtU
"""

"""
Comprehensive Ablation Study for ControlNet
Tests various hyperparameters and configurations
"""

import os
import torch
import numpy as np
from dataclasses import dataclass
from typing import List, Dict
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
import json

# ============================================================================
# ABLATION CONFIGURATIONS
# ============================================================================

@dataclass
class AblationExperiment:
    """Single ablation experiment configuration"""
    name: str
    description: str
    config_overrides: Dict

class AblationSuite:
    """Complete suite of ablation experiments"""

    @staticmethod
    def get_all_experiments():
        """Define all ablation experiments"""
        experiments = []

        # 1. Learning Rate Ablation
        for lr in [1e-4, 1e-5, 2e-6, 1e-6]:
            experiments.append(AblationExperiment(
                name=f"lr_{lr}",
                description=f"Learning Rate = {lr}",
                config_overrides={'learning_rate': lr}
            ))

        # 2. Epoch Ablation
        for epochs in [1, 2, 3, 5]:
            experiments.append(AblationExperiment(
                name=f"epochs_{epochs}",
                description=f"Training for {epochs} epoch(s)",
                config_overrides={'num_epochs': epochs}
            ))

        # 3. Batch Size Ablation
        for bs in [2, 4, 8]:
            experiments.append(AblationExperiment(
                name=f"batch_{bs}",
                description=f"Batch Size = {bs}",
                config_overrides={'train_batch_size': bs}
            ))

        # 4. ControlNet Conditioning Scale
        for scale in [0.5, 0.75, 1.0, 1.25, 1.5]:
            experiments.append(AblationExperiment(
                name=f"scale_{scale}",
                description=f"Conditioning Scale = {scale}",
                config_overrides={'controlnet_conditioning_scale': scale}
            ))

        # 5. With vs Without Text Prompts
        experiments.append(AblationExperiment(
            name="no_text_prompts",
            description="Training without text prompts",
            config_overrides={'use_text_prompts': False}
        ))

        experiments.append(AblationExperiment(
            name="with_text_prompts",
            description="Training with text prompts",
            config_overrides={'use_text_prompts': True}
        ))

        # 6. Gradient Accumulation
        for ga in [1, 2, 4]:
            experiments.append(AblationExperiment(
                name=f"grad_accum_{ga}",
                description=f"Gradient Accumulation Steps = {ga}",
                config_overrides={'gradient_accumulation_steps': ga}
            ))

        # 7. Optimizer Settings
        experiments.append(AblationExperiment(
            name="adam_default",
            description="Adam with default betas",
            config_overrides={
                'adam_beta1': 0.9,
                'adam_beta2': 0.999
            }
        ))

        experiments.append(AblationExperiment(
            name="adam_aggressive",
            description="Adam with aggressive betas",
            config_overrides={
                'adam_beta1': 0.95,
                'adam_beta2': 0.999
            }
        ))

        return experiments

# ============================================================================
# ABLATION RUNNER
# ============================================================================

class AblationRunner:
    """Run and manage ablation experiments"""

    def __init__(self, base_config, pkl_path, output_dir="./ablation_results"):
        self.base_config = base_config
        self.pkl_path = pkl_path
        self.output_dir = output_dir
        self.results = []

        os.makedirs(output_dir, exist_ok=True)

    def run_experiment(self, experiment: AblationExperiment):
        """Run a single ablation experiment"""
        print("\n" + "="*80)
        print(f"Running Experiment: {experiment.name}")
        print(f"Description: {experiment.description}")
        print("="*80 + "\n")

        # Create experiment-specific config
        config = self._create_experiment_config(experiment)

        # Create experiment directory
        exp_dir = os.path.join(self.output_dir, experiment.name)
        config.output_dir = exp_dir
        config.logging_dir = os.path.join(exp_dir, "logs")

        # Import training function
        from controlnet_training import train_controlnet

        # Run training
        try:
            monitor = train_controlnet(config, self.pkl_path)

            # Collect results
            stats = monitor.get_statistics()
            result = {
                'experiment_name': experiment.name,
                'description': experiment.description,
                'config_overrides': experiment.config_overrides,
                'statistics': stats,
                'alerts': monitor.alerts,
                'success': True
            }

        except Exception as e:
            print(f"‚ùå Experiment {experiment.name} failed: {str(e)}")
            result = {
                'experiment_name': experiment.name,
                'description': experiment.description,
                'config_overrides': experiment.config_overrides,
                'error': str(e),
                'success': False
            }

        self.results.append(result)
        return result

    def _create_experiment_config(self, experiment):
        """Create config with experiment overrides"""
        import copy
        config = copy.deepcopy(self.base_config)

        for key, value in experiment.config_overrides.items():
            setattr(config, key, value)

        return config

    def run_all_experiments(self, experiments: List[AblationExperiment]):
        """Run all ablation experiments"""
        print(f"\nüöÄ Starting {len(experiments)} ablation experiments...")

        for i, exp in enumerate(experiments, 1):
            print(f"\n[{i}/{len(experiments)}] Running: {exp.name}")
            self.run_experiment(exp)

        # Save all results
        self.save_results()

        # Generate comparison visualizations
        self.generate_comparison_plots()

        print("\n‚úÖ All ablation experiments complete!")

    def save_results(self):
        """Save ablation results to JSON"""
        results_path = os.path.join(self.output_dir, "ablation_results.json")

        with open(results_path, 'w') as f:
            json.dump(self.results, f, indent=2)

        print(f"‚úì Ablation results saved to {results_path}")

    def generate_comparison_plots(self):
        """Generate comprehensive comparison visualizations"""
        successful_results = [r for r in self.results if r.get('success', False)]

        if not successful_results:
            print("‚ö†Ô∏è  No successful experiments to visualize")
            return

        # 1. Final Loss Comparison
        self._plot_final_loss_comparison(successful_results)

        # 2. Training Time Comparison
        self._plot_training_time_comparison(successful_results)

        # 3. Learning Rate vs Loss
        self._plot_lr_vs_loss(successful_results)

        # 4. Epoch vs Loss
        self._plot_epoch_vs_loss(successful_results)

        # 5. Comprehensive Heatmap
        self._plot_ablation_heatmap(successful_results)

    def _plot_final_loss_comparison(self, results):
        """Bar plot of final losses"""
        fig, ax = plt.subplots(figsize=(12, 6))

        names = [r['experiment_name'] for r in results]
        losses = [r['statistics'].get('current_loss', 0) for r in results]

        colors = plt.cm.viridis(np.linspace(0, 1, len(names)))
        bars = ax.bar(range(len(names)), losses, color=colors)

        ax.set_xlabel('Experiment', fontsize=12)
        ax.set_ylabel('Final Loss', fontsize=12)
        ax.set_title('Final Loss Comparison Across Ablations', fontsize=14, fontweight='bold')
        ax.set_xticks(range(len(names)))
        ax.set_xticklabels(names, rotation=45, ha='right')
        ax.grid(axis='y', alpha=0.3)

        # Add value labels on bars
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{height:.4f}',
                   ha='center', va='bottom', fontsize=8)

        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'final_loss_comparison.png'),
                   dpi=150, bbox_inches='tight')
        plt.close()
        print("‚úì Final loss comparison plot saved")

    def _plot_training_time_comparison(self, results):
        """Compare training times"""
        fig, ax = plt.subplots(figsize=(12, 6))

        names = [r['experiment_name'] for r in results]
        times = [r['statistics'].get('elapsed_time', '0:00:00') for r in results]

        # Convert times to seconds for plotting
        time_seconds = []
        for t in times:
            if isinstance(t, str):
                parts = t.split(':')
                if len(parts) == 3:
                    h, m, s = parts
                    time_seconds.append(int(h)*3600 + int(m)*60 + float(s.split('.')[0]))
                else:
                    time_seconds.append(0)
            else:
                time_seconds.append(0)

        colors = plt.cm.plasma(np.linspace(0, 1, len(names)))
        ax.barh(range(len(names)), time_seconds, color=colors)

        ax.set_ylabel('Experiment', fontsize=12)
        ax.set_xlabel('Training Time (seconds)', fontsize=12)
        ax.set_title('Training Time Comparison', fontsize=14, fontweight='bold')
        ax.set_yticks(range(len(names)))
        ax.set_yticklabels(names)
        ax.grid(axis='x', alpha=0.3)

        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'training_time_comparison.png'),
                   dpi=150, bbox_inches='tight')
        plt.close()
        print("‚úì Training time comparison plot saved")

    def _plot_lr_vs_loss(self, results):
        """Plot learning rate vs final loss"""
        lr_results = [r for r in results if 'learning_rate' in r['config_overrides']]

        if not lr_results:
            return

        fig, ax = plt.subplots(figsize=(10, 6))

        lrs = [r['config_overrides']['learning_rate'] for r in lr_results]
        losses = [r['statistics'].get('current_loss', 0) for r in lr_results]

        ax.semilogx(lrs, losses, 'o-', markersize=10, linewidth=2)

        for lr, loss in zip(lrs, losses):
            ax.annotate(f'{loss:.4f}', (lr, loss),
                       textcoords="offset points", xytext=(0,10), ha='center')

        ax.set_xlabel('Learning Rate (log scale)', fontsize=12)
        ax.set_ylabel('Final Loss', fontsize=12)
        ax.set_title('Learning Rate vs Final Loss', fontsize=14, fontweight='bold')
        ax.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'lr_vs_loss.png'),
                   dpi=150, bbox_inches='tight')
        plt.close()
        print("‚úì Learning rate vs loss plot saved")

    def _plot_epoch_vs_loss(self, results):
        """Plot epochs vs final loss"""
        epoch_results = [r for r in results if 'num_epochs' in r['config_overrides']]

        if not epoch_results:
            return

        fig, ax = plt.subplots(figsize=(10, 6))

        epochs = [r['config_overrides']['num_epochs'] for r in epoch_results]
        losses = [r['statistics'].get('current_loss', 0) for r in epoch_results]

        ax.plot(epochs, losses, 'o-', markersize=12, linewidth=2, color='steelblue')

        for ep, loss in zip(epochs, losses):
            ax.annotate(f'{loss:.4f}', (ep, loss),
                       textcoords="offset points", xytext=(0,10), ha='center')

        ax.set_xlabel('Number of Epochs', fontsize=12)
        ax.set_ylabel('Final Loss', fontsize=12)
        ax.set_title('Training Epochs vs Final Loss', fontsize=14, fontweight='bold')
        ax.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'epoch_vs_loss.png'),
                   dpi=150, bbox_inches='tight')
        plt.close()
        print("‚úì Epochs vs loss plot saved")

    def _plot_ablation_heatmap(self, results):
        """Create comprehensive heatmap of all metrics"""
        fig, ax = plt.subplots(figsize=(14, 8))

        names = [r['experiment_name'] for r in results]
        metrics = ['current_loss', 'avg_loss_last_100', 'min_loss',
                  'max_loss', 'avg_grad_norm', 'num_alerts']

        # Build matrix
        matrix = []
        for result in results:
            stats = result['statistics']
            row = [stats.get(m, 0) for m in metrics]
            matrix.append(row)

        matrix = np.array(matrix)

        # Normalize each column for better visualization
        from sklearn.preprocessing import MinMaxScaler
        scaler = MinMaxScaler()
        matrix_normalized = scaler.fit_transform(matrix)

        # Create heatmap
        sns.heatmap(matrix_normalized,
                   xticklabels=metrics,
                   yticklabels=names,
                   annot=matrix,
                   fmt='.4f',
                   cmap='YlOrRd',
                   cbar_kws={'label': 'Normalized Value'},
                   ax=ax)

        ax.set_title('Ablation Study: All Metrics Heatmap',
                    fontsize=14, fontweight='bold', pad=20)
        plt.setp(ax.get_xticklabels(), rotation=45, ha='right')
        plt.setp(ax.get_yticklabels(), rotation=0)

        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'ablation_heatmap.png'),
                   dpi=150, bbox_inches='tight')
        plt.close()
        print("‚úì Ablation heatmap saved")

# ============================================================================
# INFERENCE AND EVALUATION
# ============================================================================

class InferenceEvaluator:
    """Evaluate trained ControlNet models"""

    def __init__(self, model_path, base_model_name="runwayml/stable-diffusion-v1-5"):
        from diffusers import StableDiffusionControlNetPipeline, ControlNetModel

        print(f"Loading ControlNet from {model_path}...")
        controlnet = ControlNetModel.from_pretrained(model_path)

        self.pipe = StableDiffusionControlNetPipeline.from_pretrained(
            base_model_name,
            controlnet=controlnet,
            torch_dtype=torch.float16
        )
        self.pipe.to("cuda")
        print("‚úì Pipeline loaded")

    def generate_samples(self, conditioning_images, prompts,
                        output_dir, num_samples=4,
                        conditioning_scales=[0.5, 0.75, 1.0, 1.5]):
        """Generate samples with different conditioning scales"""
        os.makedirs(output_dir, exist_ok=True)

        results = []

        for i, (cond_img, prompt) in enumerate(zip(conditioning_images, prompts)):
            print(f"\nGenerating samples for image {i+1}/{len(conditioning_images)}")

            sample_results = {
                'conditioning_image': cond_img,
                'prompt': prompt,
                'generated_images': {}
            }

            for scale in conditioning_scales:
                print(f"  Scale {scale}...")

                image = self.pipe(
                    prompt,
                    image=cond_img,
                    num_inference_steps=50,
                    controlnet_conditioning_scale=scale
                ).images[0]

                sample_results['generated_images'][scale] = image

                # Save individual image
                save_path = os.path.join(output_dir, f"sample_{i}_scale_{scale}.png")
                image.save(save_path)

            results.append(sample_results)

            # Create comparison grid
            self._create_comparison_grid(sample_results, output_dir, i)

        print(f"\n‚úì Generated {len(results)} sample sets")
        return results

    def _create_comparison_grid(self, sample_result, output_dir, idx):
        """Create a grid comparing different conditioning scales"""
        scales = sorted(sample_result['generated_images'].keys())

        fig, axes = plt.subplots(1, len(scales) + 1, figsize=(4*(len(scales)+1), 4))

        # Show conditioning image
        axes[0].imshow(sample_result['conditioning_image'])
        axes[0].set_title('Conditioning\nImage', fontsize=10)
        axes[0].axis('off')

        # Show generated images
        for i, scale in enumerate(scales, 1):
            axes[i].imshow(sample_result['generated_images'][scale])
            axes[i].set_title(f'Scale: {scale}', fontsize=10)
            axes[i].axis('off')

        plt.suptitle(f"Prompt: {sample_result['prompt'][:50]}...",
                    fontsize=10, y=0.98)
        plt.tight_layout()

        save_path = os.path.join(output_dir, f"comparison_grid_{idx}.png")
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()

# ============================================================================
# MAIN EXECUTION
# ============================================================================

if __name__ == "__main__":
    from controlnet_training import TrainingConfig

    # Setup base configuration
    base_config = TrainingConfig()
    base_config.num_epochs = 2  # Keep experiments shorter
    base_config.train_batch_size = 4

    # Path to dataset
    pkl_path = "coco_pose_dataset.pkl"

    # Create ablation runner
    runner = AblationRunner(base_config, pkl_path)

    # Get experiments to run (you can select subset)
    all_experiments = AblationSuite.get_all_experiments()

    # For quick testing, run a subset:
    experiments_to_run = [
        exp for exp in all_experiments
        if exp.name in ['lr_1e-05', 'lr_1e-04', 'epochs_1', 'epochs_3']
    ]

    # Run experiments
    runner.run_all_experiments(experiments_to_run)

    print("\n" + "="*80)
    print("ABLATION STUDY COMPLETE")
    print("="*80)
    print(f"Results saved in: {runner.output_dir}")