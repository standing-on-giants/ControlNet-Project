# -*- coding: utf-8 -*-
"""Evaluation_Visualization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y15nd2MB2aKVw0VzBndgWys7e8CxTZtU
"""

"""
Comprehensive Evaluation and Visualization for ControlNet
Includes SSIM, LPIPS, pose alignment, and qualitative analysis
"""

import os
import torch
import torch.nn.functional as F
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import seaborn as sns
from torchvision import transforms
from skimage.metrics import structural_similarity as ssim
import json
from tqdm import tqdm

# ============================================================================
# METRICS
# ============================================================================

class MetricsCalculator:
    """Calculate various evaluation metrics"""

    def __init__(self, device='cuda'):
        self.device = device
        self.to_tensor = transforms.ToTensor()

        # Try to load LPIPS
        try:
            import lpips
            self.lpips_fn = lpips.LPIPS(net='alex').to(device)
            self.has_lpips = True
            print("✓ LPIPS loaded")
        except ImportError:
            print("⚠️  LPIPS not available. Install with: pip install lpips")
            self.has_lpips = False

    def calculate_ssim(self, img1, img2):
        """Calculate SSIM between two images"""
        # Convert to numpy if needed
        if isinstance(img1, Image.Image):
            img1 = np.array(img1)
        if isinstance(img2, Image.Image):
            img2 = np.array(img2)

        # Ensure same size
        if img1.shape != img2.shape:
            img2 = np.array(Image.fromarray(img2).resize(
                (img1.shape[1], img1.shape[0]), Image.BILINEAR
            ))

        # Calculate SSIM
        if len(img1.shape) == 3:  # Color image
            score = ssim(img1, img2, multichannel=True, channel_axis=2,
                        data_range=255)
        else:  # Grayscale
            score = ssim(img1, img2, data_range=255)

        return score

    def calculate_lpips(self, img1, img2):
        """Calculate LPIPS perceptual distance"""
        if not self.has_lpips:
            return None

        # Convert to tensors
        if isinstance(img1, Image.Image):
            img1 = self.to_tensor(img1).unsqueeze(0)
        if isinstance(img2, Image.Image):
            img2 = self.to_tensor(img2).unsqueeze(0)

        # Ensure same size
        if img1.shape != img2.shape:
            img2 = F.interpolate(img2, size=img1.shape[2:],
                                mode='bilinear', align_corners=False)

        # Normalize to [-1, 1]
        img1 = img1 * 2 - 1
        img2 = img2 * 2 - 1

        # Calculate LPIPS
        with torch.no_grad():
            dist = self.lpips_fn(img1.to(self.device), img2.to(self.device))

        return dist.item()

    def calculate_mse(self, img1, img2):
        """Calculate MSE between two images"""
        if isinstance(img1, Image.Image):
            img1 = np.array(img1)
        if isinstance(img2, Image.Image):
            img2 = np.array(img2)

        if img1.shape != img2.shape:
            img2 = np.array(Image.fromarray(img2).resize(
                (img1.shape[1], img1.shape[0]), Image.BILINEAR
            ))

        mse = np.mean((img1.astype(float) - img2.astype(float)) ** 2)
        return mse

    def calculate_psnr(self, img1, img2):
        """Calculate PSNR"""
        mse = self.calculate_mse(img1, img2)
        if mse == 0:
            return float('inf')

        max_pixel = 255.0
        psnr = 20 * np.log10(max_pixel / np.sqrt(mse))
        return psnr

# ============================================================================
# POSE CONSISTENCY EVALUATOR
# ============================================================================

class PoseConsistencyEvaluator:
    """Evaluate pose consistency between conditioning and generated images"""

    def __init__(self):
        try:
            # Try to load a pose detector (e.g., MediaPipe or OpenPose)
            import mediapipe as mp
            self.mp_pose = mp.solutions.pose
            self.pose = self.mp_pose.Pose(
                static_image_mode=True,
                model_complexity=2,
                min_detection_confidence=0.5
            )
            self.has_pose_detector = True
            print("✓ MediaPipe Pose loaded")
        except ImportError:
            print("⚠️  MediaPipe not available. Pose consistency will be limited.")
            print("   Install with: pip install mediapipe")
            self.has_pose_detector = False

    def extract_keypoints(self, image):
        """Extract pose keypoints from image"""
        if not self.has_pose_detector:
            return None

        # Convert PIL to RGB numpy array
        if isinstance(image, Image.Image):
            image = np.array(image)

        # Ensure RGB
        if image.shape[2] == 4:  # RGBA
            image = image[:, :, :3]

        # Process image
        results = self.pose.process(image)

        if results.pose_landmarks:
            # Extract keypoint coordinates
            keypoints = []
            for landmark in results.pose_landmarks.landmark:
                keypoints.append([landmark.x, landmark.y, landmark.visibility])
            return np.array(keypoints)

        return None

    def calculate_pose_similarity(self, img1, img2):
        """Calculate pose similarity between two images"""
        if not self.has_pose_detector:
            return None

        kpts1 = self.extract_keypoints(img1)
        kpts2 = self.extract_keypoints(img2)

        if kpts1 is None or kpts2 is None:
            return None

        # Calculate Euclidean distance between keypoints
        # Only consider visible keypoints (visibility > 0.5)
        visible = (kpts1[:, 2] > 0.5) & (kpts2[:, 2] > 0.5)

        if not visible.any():
            return None

        distance = np.linalg.norm(kpts1[visible, :2] - kpts2[visible, :2], axis=1)
        avg_distance = np.mean(distance)

        # Convert to similarity score (0-1, higher is better)
        similarity = np.exp(-avg_distance * 10)  # Scale factor

        return similarity

# ============================================================================
# COMPREHENSIVE EVALUATOR
# ============================================================================

class ComprehensiveEvaluator:
    """Complete evaluation pipeline"""

    def __init__(self, output_dir="./evaluation_results"):
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)

        self.metrics_calc = MetricsCalculator()
        self.pose_evaluator = PoseConsistencyEvaluator()

        self.results = []

    def evaluate_sample(self, conditioning_img, generated_img, ground_truth_img,
                       prompt, sample_id):
        """Evaluate a single generated sample"""
        result = {
            'sample_id': sample_id,
            'prompt': prompt,
            'metrics': {}
        }

        # SSIM
        ssim_score = self.metrics_calc.calculate_ssim(generated_img, ground_truth_img)
        result['metrics']['ssim'] = ssim_score

        # LPIPS
        lpips_score = self.metrics_calc.calculate_lpips(generated_img, ground_truth_img)
        if lpips_score is not None:
            result['metrics']['lpips'] = lpips_score

        # MSE and PSNR
        mse = self.metrics_calc.calculate_mse(generated_img, ground_truth_img)
        psnr = self.metrics_calc.calculate_psnr(generated_img, ground_truth_img)
        result['metrics']['mse'] = mse
        result['metrics']['psnr'] = psnr

        # Pose consistency
        pose_sim = self.pose_evaluator.calculate_pose_similarity(
            conditioning_img, generated_img
        )
        if pose_sim is not None:
            result['metrics']['pose_similarity'] = pose_sim

        self.results.append(result)

        return result

    def evaluate_batch(self, conditioning_images, generated_images,
                      ground_truth_images, prompts):
        """Evaluate a batch of samples"""
        print(f"Evaluating {len(generated_images)} samples...")

        for i in tqdm(range(len(generated_images))):
            self.evaluate_sample(
                conditioning_images[i],
                generated_images[i],
                ground_truth_images[i],
                prompts[i],
                sample_id=i
            )

        print("✓ Evaluation complete")

        # Calculate aggregate statistics
        self.calculate_aggregate_stats()

        return self.results

    def calculate_aggregate_stats(self):
        """Calculate aggregate statistics across all samples"""
        if not self.results:
            return {}

        metrics_names = list(self.results[0]['metrics'].keys())

        self.aggregate_stats = {}
        for metric in metrics_names:
            values = [r['metrics'][metric] for r in self.results
                     if metric in r['metrics'] and r['metrics'][metric] is not None]

            if values:
                self.aggregate_stats[metric] = {
                    'mean': np.mean(values),
                    'std': np.std(values),
                    'min': np.min(values),
                    'max': np.max(values),
                    'median': np.median(values)
                }

        return self.aggregate_stats

    def save_results(self):
        """Save evaluation results to JSON"""
        results_data = {
            'individual_results': self.results,
            'aggregate_stats': self.aggregate_stats
        }

        save_path = os.path.join(self.output_dir, 'evaluation_results.json')
        with open(save_path, 'w') as f:
            json.dump(results_data, f, indent=2)

        print(f"✓ Results saved to {save_path}")

    def visualize_results(self):
        """Create comprehensive visualizations"""
        if not self.results:
            print("No results to visualize")
            return

        # 1. Metrics distribution
        self._plot_metrics_distribution()

        # 2. Metrics correlation
        self._plot_metrics_correlation()

        # 3. Per-sample metrics
        self._plot_per_sample_metrics()

        # 4. Best and worst samples
        self._create_qualitative_comparison()

    def _plot_metrics_distribution(self):
        """Plot distribution of each metric"""
        metrics_names = list(self.aggregate_stats.keys())

        n_metrics = len(metrics_names)
        fig, axes = plt.subplots(2, (n_metrics + 1) // 2, figsize=(15, 8))
        axes = axes.flatten()

        for i, metric in enumerate(metrics_names):
            values = [r['metrics'][metric] for r in self.results
                     if metric in r['metrics'] and r['metrics'][metric] is not None]

            axes[i].hist(values, bins=20, edgecolor='black', alpha=0.7)
            axes[i].axvline(self.aggregate_stats[metric]['mean'],
                           color='red', linestyle='--', linewidth=2,
                           label=f"Mean: {self.aggregate_stats[metric]['mean']:.3f}")
            axes[i].set_xlabel(metric.upper(), fontsize=10)
            axes[i].set_ylabel('Frequency', fontsize=10)
            axes[i].set_title(f'{metric.upper()} Distribution', fontsize=11, fontweight='bold')
            axes[i].legend()
            axes[i].grid(alpha=0.3)

        # Hide extra subplots
        for i in range(n_metrics, len(axes)):
            axes[i].axis('off')

        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'metrics_distribution.png'),
                   dpi=150, bbox_inches='tight')
        plt.close()
        print("✓ Metrics distribution plot saved")

    def _plot_metrics_correlation(self):
        """Plot correlation between metrics"""
        metrics_names = list(self.aggregate_stats.keys())

        # Build correlation matrix
        n_metrics = len(metrics_names)
        corr_matrix = np.zeros((n_metrics, n_metrics))

        for i, metric1 in enumerate(metrics_names):
            for j, metric2 in enumerate(metrics_names):
                values1 = [r['metrics'][metric1] for r in self.results
                          if metric1 in r['metrics'] and r['metrics'][metric1] is not None]
                values2 = [r['metrics'][metric2] for r in self.results
                          if metric2 in r['metrics'] and r['metrics'][metric2] is not None]

                if len(values1) == len(values2) and len(values1) > 1:
                    corr_matrix[i, j] = np.corrcoef(values1, values2)[0, 1]

        # Plot heatmap
        fig, ax = plt.subplots(figsize=(10, 8))
        sns.heatmap(corr_matrix,
                   xticklabels=[m.upper() for m in metrics_names],
                   yticklabels=[m.upper() for m in metrics_names],
                   annot=True, fmt='.2f', cmap='coolwarm', center=0,
                   vmin=-1, vmax=1, ax=ax,
                   cbar_kws={'label': 'Correlation'})

        ax.set_title('Metrics Correlation Heatmap', fontsize=14, fontweight='bold', pad=20)
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'metrics_correlation.png'),
                   dpi=150, bbox_inches='tight')
        plt.close()
        print("✓ Metrics correlation plot saved")

    def _plot_per_sample_metrics(self):
        """Plot metrics for each sample"""
        metrics_names = list(self.aggregate_stats.keys())
        n_samples = len(self.results)

        fig, axes = plt.subplots(len(metrics_names), 1,
                                figsize=(12, 3*len(metrics_names)))

        if len(metrics_names) == 1:
            axes = [axes]

        for i, metric in enumerate(metrics_names):
            values = [r['metrics'].get(metric, 0) for r in self.results]
            sample_ids = list(range(n_samples))

            axes[i].plot(sample_ids, values, marker='o', linewidth=1, markersize=4)
            axes[i].axhline(self.aggregate_stats[metric]['mean'],
                          color='red', linestyle='--', linewidth=2,
                          label=f"Mean: {self.aggregate_stats[metric]['mean']:.3f}")
            axes[i].set_xlabel('Sample ID', fontsize=10)
            axes[i].set_ylabel(metric.upper(), fontsize=10)
            axes[i].set_title(f'{metric.upper()} per Sample',
                            fontsize=11, fontweight='bold')
            axes[i].legend()
            axes[i].grid(alpha=0.3)

        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'per_sample_metrics.png'),
                   dpi=150, bbox_inches='tight')
        plt.close()
        print("✓ Per-sample metrics plot saved")

    def _create_qualitative_comparison(self):
        """Create visualization of best and worst samples"""
        # Find best and worst samples based on SSIM
        if 'ssim' not in self.aggregate_stats:
            print("⚠️  SSIM not available for qualitative comparison")
            return

        ssim_scores = [(i, r['metrics']['ssim']) for i, r in enumerate(self.results)
                      if 'ssim' in r['metrics']]
        ssim_scores.sort(key=lambda x: x[1])

        # Get 3 worst and 3 best
        worst_ids = [idx for idx, _ in ssim_scores[:3]]
        best_ids = [idx for idx, _ in ssim_scores[-3:]]

        print(f"✓ Best samples (SSIM): {[ssim_scores[i][1] for i in range(-3, 0)]}")
        print(f"✓ Worst samples (SSIM): {[ssim_scores[i][1] for i in range(3)]}")

        # Note: This would require access to actual images
        # You would need to pass images to the evaluator for this to work
        print("✓ Qualitative comparison identified (visualization requires images)")

# ============================================================================
# FAILURE ANALYSIS
# ============================================================================

class FailureAnalyzer:
    """Analyze failure cases"""

    def __init__(self, results, threshold_ssim=0.5):
        self.results = results
        self.threshold_ssim = threshold_ssim
        self.failure_cases = []

    def identify_failures(self):
        """Identify failure cases based on metrics"""
        for result in self.results:
            is_failure = False
            failure_reasons = []

            # Check SSIM
            if 'ssim' in result['metrics']:
                if result['metrics']['ssim'] < self.threshold_ssim:
                    is_failure = True
                    failure_reasons.append(
                        f"Low SSIM: {result['metrics']['ssim']:.3f}"
                    )

            # Check pose similarity
            if 'pose_similarity' in result['metrics']:
                if result['metrics']['pose_similarity'] < 0.5:
                    is_failure = True
                    failure_reasons.append(
                        f"Low pose similarity: {result['metrics']['pose_similarity']:.3f}"
                    )

            if is_failure:
                self.failure_cases.append({
                    'sample_id': result['sample_id'],
                    'prompt': result['prompt'],
                    'metrics': result['metrics'],
                    'reasons': failure_reasons
                })

        print(f"Identified {len(self.failure_cases)} failure cases "
              f"({len(self.failure_cases)/len(self.results)*100:.1f}%)")

        return self.failure_cases

    def categorize_failures(self):
        """Categorize failure types"""
        categories = {
            'low_ssim': [],
            'low_pose_similarity': [],
            'high_lpips': []
        }

        for failure in self.failure_cases:
            if any('SSIM' in reason for reason in failure['reasons']):
                categories['low_ssim'].append(failure)
            if any('pose' in reason for reason in failure['reasons']):
                categories['low_pose_similarity'].append(failure)
            if 'lpips' in failure['metrics'] and failure['metrics']['lpips'] > 0.5:
                categories['high_lpips'].append(failure)

        print("\nFailure Categories:")
        for category, cases in categories.items():
            print(f"  {category}: {len(cases)} cases")

        return categories

    def generate_failure_report(self, output_path):
        """Generate detailed failure analysis report"""
        report = {
            'total_samples': len(self.results),
            'num_failures': len(self.failure_cases),
            'failure_rate': len(self.failure_cases) / len(self.results),
            'failure_cases': self.failure_cases,
            'categories': self.categorize_failures()
        }

        with open(output_path, 'w') as f:
            json.dump(report, f, indent=2)

        print(f"✓ Failure report saved to {output_path}")

# ============================================================================
# MAIN USAGE EXAMPLE
# ============================================================================

if __name__ == "__main__":
    # Example usage
    evaluator = ComprehensiveEvaluator()

    # Load your generated samples, conditioning images, and ground truth
    conditioning_images = "../coco_pose_dataset/2_pose.png"
    generated_images = "../results/generated.png"
    ground_truth_images = "../coco_pose_dataset/2.png"
    prompts = "an astronaut in space suit, futuristic, highly detailed, photorealistic"

    # Run evaluation
    results = evaluator.evaluate_batch(
        conditioning_images,
        generated_images,
        ground_truth_images,
        prompts
    )

    # Visualize
    evaluator.visualize_results()
    evaluator.save_results()

    # Failure analysis
    analyzer = FailureAnalyzer(evaluator.results)
    analyzer.identify_failures()
    analyzer.generate_failure_report('failure_report.json')

    print("✓ Evaluation framework ready")