# -*- coding: utf-8 -*-
"""Gradio_Demo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y15nd2MB2aKVw0VzBndgWys7e8CxTZtU
"""

"""
Interactive Gradio Demo for ControlNet Pose-to-Person Generation
"""

import os
import torch
import numpy as np
from PIL import Image
import gradio as gr
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel
import cv2

# ============================================================================
# POSE EXTRACTION (Optional - for user-uploaded images)
# ============================================================================

class PoseExtractor:
    """Extract pose skeleton from images"""

    def __init__(self):
        try:
            import mediapipe as mp
            self.mp_pose = mp.solutions.pose
            self.mp_drawing = mp.solutions.drawing_utils
            self.pose = self.mp_pose.Pose(
                static_image_mode=True,
                model_complexity=2,
                min_detection_confidence=0.5
            )
            self.available = True
            print("✓ MediaPipe Pose loaded")
        except ImportError:
            print("⚠️  MediaPipe not available")
            self.available = False

    def extract_pose_skeleton(self, image):
        """Extract pose skeleton from image"""
        if not self.available:
            return None

        # Convert to RGB
        if isinstance(image, Image.Image):
            image = np.array(image)

        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) if len(image.shape) == 3 else image

        # Process
        results = self.pose.process(image_rgb)

        if not results.pose_landmarks:
            return None

        # Create blank canvas
        skeleton_img = np.zeros_like(image)

        # Draw skeleton
        self.mp_drawing.draw_landmarks(
            skeleton_img,
            results.pose_landmarks,
            self.mp_pose.POSE_CONNECTIONS,
            landmark_drawing_spec=self.mp_drawing.DrawingSpec(
                color=(0, 255, 0), thickness=3, circle_radius=5
            ),
            connection_drawing_spec=self.mp_drawing.DrawingSpec(
                color=(255, 0, 0), thickness=2
            )
        )

        return Image.fromarray(skeleton_img)

# ============================================================================
# DEMO APP
# ============================================================================

class ControlNetDemo:
    """Gradio demo application"""

    def __init__(self, model_path, base_model="runwayml/stable-diffusion-v1-5"):
        print("Loading ControlNet model...")

        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"Using device: {self.device}")

        # Load model
        try:
            controlnet = ControlNetModel.from_pretrained(model_path)

            self.pipe = StableDiffusionControlNetPipeline.from_pretrained(
                base_model,
                controlnet=controlnet,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32
            )

            self.pipe = self.pipe.to(self.device)

            # Enable optimizations
            if self.device == "cuda":
                try:
                    self.pipe.enable_xformers_memory_efficient_attention()
                    print("✓ xformers enabled")
                except:
                    print("⚠️  xformers not available")

            print("✓ Model loaded successfully")

        except Exception as e:
            print(f"❌ Failed to load model: {e}")
            raise

        # Initialize pose extractor
        self.pose_extractor = PoseExtractor()

    def generate(self, pose_image, prompt, num_steps, guidance_scale,
                conditioning_scale, seed):
        """Generate image from pose"""

        if pose_image is None:
            return None, "Please provide a pose image"

        # Set seed for reproducibility
        if seed == -1:
            seed = np.random.randint(0, 2**32 - 1)

        generator = torch.Generator(device=self.device).manual_seed(seed)

        # Ensure pose image is PIL Image
        if isinstance(pose_image, np.ndarray):
            pose_image = Image.fromarray(pose_image)

        # Resize to 512x512
        pose_image = pose_image.resize((512, 512), Image.BILINEAR)

        try:
            # Generate
            with torch.no_grad():
                output = self.pipe(
                    prompt=prompt,
                    image=pose_image,
                    num_inference_steps=num_steps,
                    guidance_scale=guidance_scale,
                    controlnet_conditioning_scale=conditioning_scale,
                    generator=generator
                )

            generated_image = output.images[0]

            info = f"Generated with seed: {seed}"

            return generated_image, info

        except Exception as e:
            return None, f"Generation failed: {str(e)}"

    def extract_and_generate(self, input_image, prompt, num_steps,
                            guidance_scale, conditioning_scale, seed):
        """Extract pose from image and generate"""

        if input_image is None:
            return None, None, "Please provide an input image"

        # Extract pose
        pose_skeleton = self.pose_extractor.extract_pose_skeleton(input_image)

        if pose_skeleton is None:
            return None, None, "Failed to extract pose from image"

        # Generate
        generated, info = self.generate(
            pose_skeleton, prompt, num_steps,
            guidance_scale, conditioning_scale, seed
        )

        return pose_skeleton, generated, info

    def create_interface(self):
        """Create Gradio interface"""

        with gr.Blocks(title="ControlNet: Pose-to-Person Generation") as demo:
            gr.Markdown("""
            # ControlNet: Pose-to-Person Generation
            Generate realistic person images from pose skeletons using ControlNet.

            **Two modes:**
            1. **Direct Mode**: Upload a pose skeleton directly
            2. **Extract Mode**: Upload a person image and extract pose automatically
            """)

            with gr.Tabs():
                # Tab 1: Direct generation from pose
                with gr.Tab("Direct Generation"):
                    with gr.Row():
                        with gr.Column():
                            pose_input = gr.Image(
                                label="Pose Skeleton",
                                type="pil"
                            )
                            prompt_input = gr.Textbox(
                                label="Text Prompt",
                                placeholder="A person wearing blue jeans and a red shirt",
                                value="A person standing"
                            )

                            with gr.Accordion("Advanced Settings", open=False):
                                steps_slider = gr.Slider(
                                    minimum=20, maximum=100, value=50, step=1,
                                    label="Inference Steps"
                                )
                                guidance_slider = gr.Slider(
                                    minimum=1, maximum=20, value=7.5, step=0.5,
                                    label="Guidance Scale"
                                )
                                conditioning_slider = gr.Slider(
                                    minimum=0.0, maximum=2.0, value=1.0, step=0.1,
                                    label="Conditioning Scale"
                                )
                                seed_input = gr.Number(
                                    label="Seed (-1 for random)",
                                    value=-1,
                                    precision=0
                                )

                            generate_btn = gr.Button("Generate", variant="primary")

                        with gr.Column():
                            output_image = gr.Image(label="Generated Image")
                            output_info = gr.Textbox(label="Generation Info")

                    generate_btn.click(
                        fn=self.generate,
                        inputs=[
                            pose_input, prompt_input, steps_slider,
                            guidance_slider, conditioning_slider, seed_input
                        ],
                        outputs=[output_image, output_info]
                    )

                # Tab 2: Extract pose and generate
                with gr.Tab("Extract & Generate"):
                    with gr.Row():
                        with gr.Column():
                            image_input = gr.Image(
                                label="Input Person Image",
                                type="numpy"
                            )
                            prompt_input2 = gr.Textbox(
                                label="Text Prompt",
                                placeholder="A person wearing blue jeans and a red shirt",
                                value="A person standing"
                            )

                            with gr.Accordion("Advanced Settings", open=False):
                                steps_slider2 = gr.Slider(
                                    minimum=20, maximum=100, value=50, step=1,
                                    label="Inference Steps"
                                )
                                guidance_slider2 = gr.Slider(
                                    minimum=1, maximum=20, value=7.5, step=0.5,
                                    label="Guidance Scale"
                                )
                                conditioning_slider2 = gr.Slider(
                                    minimum=0.0, maximum=2.0, value=1.0, step=0.1,
                                    label="Conditioning Scale"
                                )
                                seed_input2 = gr.Number(
                                    label="Seed (-1 for random)",
                                    value=-1,
                                    precision=0
                                )

                            extract_btn = gr.Button("Extract & Generate", variant="primary")

                        with gr.Column():
                            extracted_pose = gr.Image(label="Extracted Pose")
                            output_image2 = gr.Image(label="Generated Image")
                            output_info2 = gr.Textbox(label="Generation Info")

                    extract_btn.click(
                        fn=self.extract_and_generate,
                        inputs=[
                            image_input, prompt_input2, steps_slider2,
                            guidance_slider2, conditioning_slider2, seed_input2
                        ],
                        outputs=[extracted_pose, output_image2, output_info2]
                    )

            gr.Markdown("""
            ### Tips:
            - **Guidance Scale**: Higher values follow the prompt more closely (7-8 works well)
            - **Conditioning Scale**: Controls how strictly to follow the pose (1.0 is default)
            - **Inference Steps**: More steps = better quality but slower (50 is good balance)
            - Use the same seed to reproduce results

            ### Examples of good prompts:
            - "A person wearing a red jacket and blue jeans"
            - "A woman in a yoga pose wearing sportswear"
            - "A man in a business suit standing confidently"
            - "A person dancing in colorful clothes"
            """)

        return demo

# ============================================================================
# MAIN
# ============================================================================

def launch_demo(model_path, share=False, server_name="0.0.0.0", server_port=7860):
    """Launch the Gradio demo"""

    print("\n" + "="*80)
    print("LAUNCHING CONTROLNET DEMO")
    print("="*80 + "\n")

    # Create demo app
    app = ControlNetDemo(model_path)

    # Create interface
    demo = app.create_interface()

    # Launch
    print(f"\nLaunching demo on {server_name}:{server_port}")
    print(f"Share: {share}")

    demo.launch(
        share=share,
        server_name=server_name,
        server_port=server_port,
        show_error=True
    )

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='Launch ControlNet Demo')
    parser.add_argument(
        '--model-path',
        type=str,
        required=True,
        help='Path to trained ControlNet model'
    )
    parser.add_argument(
        '--share',
        action='store_true',
        help='Create public share link'
    )
    parser.add_argument(
        '--port',
        type=int,
        default=7860,
        help='Server port'
    )

    args = parser.parse_args()

    launch_demo(
        model_path=args.model_path,
        share=args.share,
        server_port=args.port
    )