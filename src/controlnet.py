# -*- coding: utf-8 -*-
"""ControlNet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zRpGry3g52mLoXwo0pThf9OLvfR1g1-y
"""

# ============================================
# COMPLETE ControlNet for Human Pose to Person Image
# Single code block for Google Colab
# ============================================

!pip install -q diffusers transformers accelerate xformers opencv-python mediapipe pillow matplotlib datasets torch torchvision huggingface_hub controlnet_aux gradio

import torch
import numpy as np
import cv2
from PIL import Image, ImageDraw
import matplotlib.pyplot as plt
import mediapipe as mp
from datasets import load_dataset
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler
from diffusers import DDPMScheduler
from diffusers.optimization import get_cosine_schedule_with_warmup
from diffusers import ControlNetModel, StableDiffusionPipeline
import torch.nn.functional as F
from torch.utils.data import DataLoader
import gradio as gr
import os
from tqdm.auto import tqdm
from datetime import datetime

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# ============================================
# 1. DATA PREPARATION - Download and process dataset
# ============================================

print("Setting up dataset...")

def create_pose_skeleton_image(image_size=512, keypoints=None):
    """Create pose skeleton visualization from keypoints"""
    img = Image.new('RGB', (image_size, image_size), color=(0, 0, 0))
    draw = ImageDraw.Draw(img)

    # Define connections between keypoints (MediaPipe pose landmarks)
    connections = [
        (0, 1), (1, 2), (2, 3), (3, 7),  # Face
        (0, 4), (4, 5), (5, 6), (6, 8),  # Face
        (9, 10),  # Shoulders
        (11, 12), (11, 13), (13, 15),  # Left arm
        (12, 14), (14, 16),  # Right arm
        (11, 23), (12, 24),  # Torso
        (23, 24), (23, 25), (25, 27),  # Left leg
        (24, 26), (26, 28),  # Right leg
    ]

    if keypoints is None:
        # Create a simple default pose (T-pose)
        keypoints = []
        # Head
        keypoints.append((image_size // 2, image_size // 4))
        # Shoulders
        keypoints.append((image_size // 3, image_size // 3))
        keypoints.append((2 * image_size // 3, image_size // 3))
        # Elbows
        keypoints.append((image_size // 4, image_size // 2))
        keypoints.append((3 * image_size // 4, image_size // 2))
        # Wrists
        keypoints.append((image_size // 6, 2 * image_size // 3))
        keypoints.append((5 * image_size // 6, 2 * image_size // 3))
        # Hips
        keypoints.append((image_size // 2 - 20, image_size // 2))
        keypoints.append((image_size // 2 + 20, image_size // 2))
        # Knees
        keypoints.append((image_size // 2 - 20, 3 * image_size // 4))
        keypoints.append((image_size // 2 + 20, 3 * image_size // 4))
        # Ankles
        keypoints.append((image_size // 2 - 20, 7 * image_size // 8))
        keypoints.append((image_size // 2 + 20, 7 * image_size // 8))

    # Draw connections
    for start_idx, end_idx in connections:
        if start_idx < len(keypoints) and end_idx < len(keypoints):
            start = keypoints[start_idx]
            end = keypoints[end_idx]
            draw.line([start, end], fill=(255, 255, 255), width=3)

    # Draw keypoints
    for point in keypoints:
        x, y = point
        draw.ellipse([x-5, y-5, x+5, y+5], fill=(255, 255, 255))

    return img

def create_synthetic_dataset(num_samples=100):
    """Create synthetic dataset with different poses"""
    dataset = []
    image_size = 512

    # Different pose configurations
    poses = [
        # T-pose
        lambda t: [
            (image_size // 2, image_size // 4),  # Head
            (image_size // 3, image_size // 3), (2 * image_size // 3, image_size // 3),  # Shoulders
            (image_size // 4, image_size // 2), (3 * image_size // 4, image_size // 2),  # Elbows
            (image_size // 6, 2 * image_size // 3), (5 * image_size // 6, 2 * image_size // 3),  # Wrists
            (image_size // 2 - 20, image_size // 2), (image_size // 2 + 20, image_size // 2),  # Hips
            (image_size // 2 - 20, 3 * image_size // 4), (image_size // 2 + 20, 3 * image_size // 4),  # Knees
            (image_size // 2 - 20, 7 * image_size // 8), (image_size // 2 + 20, 7 * image_size // 8),  # Ankles
        ],
        # Walking pose
        lambda t: [
            (image_size // 2, image_size // 4),
            (image_size // 2 - 30, image_size // 3), (image_size // 2 + 30, image_size // 3),
            (image_size // 2 - 50, image_size // 2), (image_size // 2 + 50, image_size // 2),
            (image_size // 2 - 60, 2 * image_size // 3), (image_size // 2 + 40, 2 * image_size // 3),
            (image_size // 2 - 30, image_size // 2), (image_size // 2 + 30, image_size // 2),
            (image_size // 2 - 30, 3 * image_size // 4), (image_size // 2 + 30, 3 * image_size // 4),
            (image_size // 2 - 30, 7 * image_size // 8 - 20), (image_size // 2 + 30, 7 * image_size // 8 + 20),
        ],
        # Sitting pose
        lambda t: [
            (image_size // 2, image_size // 4),
            (image_size // 2 - 20, image_size // 3), (image_size // 2 + 20, image_size // 3),
            (image_size // 2 - 40, image_size // 2), (image_size // 2 + 40, image_size // 2),
            (image_size // 2 - 50, 2 * image_size // 3), (image_size // 2 + 30, 2 * image_size // 3),
            (image_size // 2 - 30, 3 * image_size // 5), (image_size // 2 + 30, 3 * image_size // 5),
            (image_size // 2 - 30, 4 * image_size // 5), (image_size // 2 + 30, 4 * image_size // 5),
            (image_size // 2 - 30, 4 * image_size // 5), (image_size // 2 + 30, 4 * image_size // 5),
        ],
    ]

    # Prompts for different personas
    prompts = [
        "a professional business person in formal attire",
        "an athlete in sportswear",
        "a casual person in everyday clothes",
        "a superhero in costume",
        "a historical figure in traditional clothing",
        "a futuristic cyberpunk character",
        "a medieval knight in armor",
        "a beachgoer in summer clothes",
        "a winter sports athlete in cold weather gear",
        "a dancer in performance costume",
    ]

    for i in range(num_samples):
        pose_idx = i % len(poses)
        prompt_idx = i % len(prompts)

        # Create pose skeleton
        pose_func = poses[pose_idx]
        keypoints = pose_func(i)
        pose_image = create_pose_skeleton_image(image_size, keypoints)

        # Create a simple color background for the "person" image
        bg_color = np.random.randint(50, 200, 3)
        person_image = Image.new('RGB', (image_size, image_size),
                                color=tuple(bg_color.tolist()))

        # Add some simple shapes to simulate a person (for training)
        draw = ImageDraw.Draw(person_image)

        # Draw head
        head_center = keypoints[0]
        draw.ellipse([head_center[0]-30, head_center[1]-30,
                      head_center[0]+30, head_center[1]+30],
                     fill=tuple(np.random.randint(150, 250, 3).tolist()))

        # Draw torso
        if len(keypoints) > 8:
            shoulder_left = keypoints[1]
            shoulder_right = keypoints[2]
            hip_left = keypoints[7]
            hip_right = keypoints[8]
            draw.polygon([shoulder_left, shoulder_right, hip_right, hip_left],
                         fill=tuple(np.random.randint(100, 200, 3).tolist()))

        dataset.append({
            'image': person_image,
            'conditioning_image': pose_image,
            'prompt': prompts[prompt_idx]
        })

    return dataset

# Create synthetic dataset
print("Creating synthetic dataset...")
synthetic_dataset = create_synthetic_dataset(num_samples=50)

# Show sample
fig, axes = plt.subplots(1, 2, figsize=(10, 5))
axes[0].imshow(synthetic_dataset[0]['conditioning_image'])
axes[0].set_title('Pose Input')
axes[0].axis('off')
axes[1].imshow(synthetic_dataset[0]['image'])
axes[1].set_title('Generated Person')
axes[1].axis('off')
plt.suptitle('Sample Training Pair', fontsize=14)
plt.tight_layout()
plt.show()

# ============================================
# 2. CONTROLNET TRAINING
# ============================================

class ControlNetTrainer:
    def __init__(self):
        self.model_id = "runwayml/stable-diffusion-v1-5"
        self.image_size = 512
        self.batch_size = 2  # Reduced for Colab memory
        self.num_epochs = 3  # Reduced for demo
        self.gradient_accumulation_steps = 4
        self.learning_rate = 1e-5

        # Load pretrained models
        print("Loading pretrained models...")
        self.controlnet = ControlNetModel.from_pretrained(
            "lllyasviel/sd-controlnet-openpose",
            torch_dtype=torch.float16
        )

        # Initialize with SD weights
        self.pipe = StableDiffusionControlNetPipeline.from_pretrained(
            self.model_id,
            controlnet=self.controlnet,
            torch_dtype=torch.float16,
            safety_checker=None,
            requires_safety_checker=False
        )

        # Freeze the main UNet
        self.pipe.unet.requires_grad_(False)
        self.pipe.vae.requires_grad_(False)
        self.pipe.text_encoder.requires_grad_(False)

        # Only train ControlNet
        self.controlnet.train()
        self.controlnet = self.controlnet.to(device)
        self.pipe.unet = self.pipe.unet.to(device)
        self.pipe.vae = self.pipe.vae.to(device)
        self.pipe.text_encoder = self.pipe.text_encoder.to(device)

        # Optimizer
        self.optimizer = torch.optim.AdamW(
            self.controlnet.parameters(),
            lr=self.learning_rate,
            weight_decay=1e-2
        )

        # Scheduler
        self.noise_scheduler = DDPMScheduler.from_pretrained(
            self.model_id,
            subfolder="scheduler"
        )

        self.lr_scheduler = get_cosine_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=100,
            num_training_steps=self.num_epochs * len(synthetic_dataset) // self.batch_size
        )

    def preprocess_image(self, image):
        """Preprocess image for VAE"""
        if isinstance(image, Image.Image):
            image = image.convert("RGB")
            image = image.resize((self.image_size, self.image_size))
            image = np.array(image)

        image = (image / 127.5 - 1.0).astype(np.float32)
        image = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0)
        return image

    def preprocess_condition(self, condition_image):
        """Preprocess condition image"""
        if isinstance(condition_image, Image.Image):
            condition_image = condition_image.convert("RGB")
            condition_image = condition_image.resize((self.image_size, self.image_size))
            condition_image = np.array(condition_image)

        condition_image = (condition_image / 255.0).astype(np.float32)
        condition_image = torch.from_numpy(condition_image).permute(2, 0, 1).unsqueeze(0)
        return condition_image

    def train_step(self, batch):
        """Single training step"""
        images = batch['images'].to(device, dtype=torch.float16)
        conditions = batch['conditions'].to(device, dtype=torch.float16)
        prompts = batch['prompts']

        # Encode images with VAE
        with torch.no_grad():
            latents = self.pipe.vae.encode(images).latent_dist.sample()
            latents = latents * self.pipe.vae.config.scaling_factor

        # Sample noise
        noise = torch.randn_like(latents)
        timesteps = torch.randint(
            0, self.noise_scheduler.config.num_train_timesteps,
            (latents.shape[0],), device=device
        ).long()

        # Add noise
        noisy_latents = self.noise_scheduler.add_noise(latents, noise, timesteps)

        # Encode text
        with torch.no_grad():
            text_inputs = self.pipe.tokenizer(
                prompts,
                padding="max_length",
                max_length=self.pipe.tokenizer.model_max_length,
                truncation=True,
                return_tensors="pt"
            )
            text_embeddings = self.pipe.text_encoder(
                text_inputs.input_ids.to(device)
            )[0]

        # ControlNet forward
        down_block_res_samples, mid_block_res_sample = self.controlnet(
            noisy_latents,
            timesteps,
            encoder_hidden_states=text_embeddings,
            controlnet_cond=conditions,
            return_dict=False,
        )

        # UNet forward
        noise_pred = self.pipe.unet(
            noisy_latents,
            timesteps,
            encoder_hidden_states=text_embeddings,
            down_block_additional_residuals=down_block_res_samples,
            mid_block_additional_residual=mid_block_res_sample,
        ).sample

        # Compute loss
        loss = F.mse_loss(noise_pred, noise)
        return loss

    def train(self, dataset):
        """Main training loop"""
        print("Starting training...")

        # Create data loader
        train_data = []
        for item in dataset:
            image_tensor = self.preprocess_image(item['image'])
            condition_tensor = self.preprocess_condition(item['conditioning_image'])

            train_data.append({
                'images': image_tensor.squeeze(0),
                'conditions': condition_tensor.squeeze(0),
                'prompts': item['prompt']
            })

        dataloader = DataLoader(
            train_data,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=0
        )

        # Training loop
        global_step = 0
        for epoch in range(self.num_epochs):
            print(f"\nEpoch {epoch + 1}/{self.num_epochs}")

            self.controlnet.train()
            total_loss = 0

            for batch_idx, batch in enumerate(tqdm(dataloader, desc="Training")):
                # Forward and backward
                loss = self.train_step(batch)
                loss = loss / self.gradient_accumulation_steps
                loss.backward()

                if (batch_idx + 1) % self.gradient_accumulation_steps == 0:
                    self.optimizer.step()
                    self.lr_scheduler.step()
                    self.optimizer.zero_grad()
                    global_step += 1

                total_loss += loss.item()

                # Log every 5 steps
                if batch_idx % 5 == 0:
                    print(f"Step {batch_idx}, Loss: {loss.item():.4f}")

            avg_loss = total_loss / len(dataloader)
            print(f"Epoch {epoch + 1} Average Loss: {avg_loss:.4f}")

            # Save checkpoint
            torch.save(self.controlnet.state_dict(),
                      f"controlnet_pose_epoch_{epoch+1}.pth")

        print("Training completed!")
        return self.controlnet

# Train the model (simplified for demo)
print("\n" + "="*50)
print("TRAINING CONTROLNET")
print("="*50)

# For demo purposes, we'll skip actual training and load a pretrained model
# In a real scenario, uncomment the training code below

"""
trainer = ControlNetTrainer()
trained_controlnet   = trainer.train(synthetic_dataset)
"""

print("Note: Skipping actual training for demo speed.")
print("Loading pretrained ControlNet for inference...")

# ============================================
# 3. INFERENCE WITH PRETRAINED MODEL
# ============================================

class PoseToPersonGenerator:
    def __init__(self):
        print("Loading inference pipeline...")

        # Load pretrained ControlNet for pose
        self.controlnet = ControlNetModel.from_pretrained(
            "lllyasviel/sd-controlnet-openpose",
            torch_dtype=torch.float16
        )

        # Load Stable Diffusion
        self.pipe = StableDiffusionControlNetPipeline.from_pretrained(
            "runwayml/stable-diffusion-v1-5",
            controlnet=self.controlnet,
            torch_dtype=torch.float16,
            safety_checker=None,
            requires_safety_checker=False
        )

        # Optimize for speed
        self.pipe.scheduler = UniPCMultistepScheduler.from_config(self.pipe.scheduler.config)

        if torch.cuda.is_available():
            self.pipe.enable_xformers_memory_efficient_attention()
            self.pipe.enable_model_cpu_offload()
        else:
            self.pipe = self.pipe.to("cpu")

        self.pipe.enable_attention_slicing()

        # Define pose templates for demo
        self.pose_templates = {
            "T-pose": create_pose_skeleton_image(512, [
                (256, 128),  # Head
                (170, 170), (342, 170),  # Shoulders
                (128, 256), (384, 256),  # Elbows
                (85, 341), (427, 341),  # Wrists
                (236, 256), (276, 256),  # Hips
                (236, 384), (276, 384),  # Knees
                (236, 448), (276, 448),  # Ankles
            ]),
            "Walking": create_pose_skeleton_image(512, [
                (256, 128),  # Head
                (226, 170), (286, 170),  # Shoulders
                (206, 256), (306, 256),  # Elbows
                (196, 341), (326, 341),  # Wrists
                (226, 256), (286, 256),  # Hips
                (226, 384), (286, 384),  # Knees
                (226, 428), (286, 468),  # Ankles (one higher)
            ]),
            "Sitting": create_pose_skeleton_image(512, [
                (256, 128),  # Head
                (236, 170), (276, 170),  # Shoulders
                (216, 256), (296, 256),  # Elbows
                (206, 341), (306, 341),  # Wrists
                (236, 307), (276, 307),  # Hips (higher up)
                (236, 410), (276, 410),  # Knees
                (236, 410), (276, 410),  # Ankles (same as knees for sitting)
            ]),
            "Dancing": create_pose_skeleton_image(512, [
                (256, 128),  # Head
                (206, 150), (306, 190),  # Shoulders (asymmetric)
                (176, 220), (336, 260),  # Elbows
                (156, 290), (366, 330),  # Wrists
                (216, 276), (296, 296),  # Hips
                (216, 376), (296, 396),  # Knees
                (216, 446), (296, 466),  # Ankles
            ]),
        }

    def generate_from_pose(self, pose_image, prompt, num_images=1,
                          guidance_scale=7.5, controlnet_conditioning_scale=1.0):
        """Generate person image from pose skeleton"""
        # Ensure pose image is right format
        if isinstance(pose_image, np.ndarray):
            pose_image = Image.fromarray(pose_image)

        pose_image = pose_image.convert("RGB").resize((512, 512))

        # Generate
        generator = torch.Generator(device="cuda").manual_seed(42) if torch.cuda.is_available() else None

        images = self.pipe(
            prompt,
            pose_image,
            negative_prompt="blurry, ugly, deformed, disfigured, poor details, bad anatomy",
            num_inference_steps=20,
            generator=generator,
            num_images_per_prompt=num_images,
            guidance_scale=guidance_scale,
            controlnet_conditioning_scale=controlnet_conditioning_scale
        ).images

        return images

    def create_demo_interface(self):
        """Create Gradio interface for demo"""

        def generate_pose_image(pose_type):
            return self.pose_templates[pose_type]

        def process_generation(pose_type, prompt, guidance_scale, conditioning_scale):
            pose_image = self.pose_templates[pose_type]
            generated = self.generate_from_pose(
                pose_image,
                prompt,
                guidance_scale=guidance_scale,
                controlnet_conditioning_scale=conditioning_scale
            )[0]
            return pose_image, generated

        # Create interface
        with gr.Blocks(title="ControlNet: Pose to Person Generation") as demo:
            gr.Markdown("# ðŸ•º ControlNet Pose to Person Generator")
            gr.Markdown("Generate realistic person images from pose skeletons")

            with gr.Row():
                with gr.Column():
                    pose_type = gr.Dropdown(
                        choices=list(self.pose_templates.keys()),
                        value="T-pose",
                        label="Select Pose Template"
                    )
                    prompt = gr.Textbox(
                        label="Description",
                        value="a professional business person in formal attire, high quality, detailed",
                        placeholder="Describe the person you want to generate..."
                    )
                    guidance_scale = gr.Slider(1, 20, value=7.5, label="Guidance Scale")
                    conditioning_scale = gr.Slider(0, 2, value=1.0, label="ControlNet Conditioning Scale")
                    generate_btn = gr.Button("Generate Person", variant="primary")

                with gr.Column():
                    pose_display = gr.Image(label="Input Pose", interactive=False)
                    output_display = gr.Image(label="Generated Person", interactive=False)

            # Examples
            gr.Markdown("### Example Prompts:")
            examples = gr.Examples(
                examples=[
                    ["T-pose", "an astronaut in space suit, futuristic, highly detailed, photorealistic"],
                    ["Walking", "a fashion model on runway, elegant dress, studio lighting, vogue magazine"],
                    ["Sitting", "a CEO in office, business suit, professional, confident, boardroom"],
                    ["Dancing", "a ballet dancer on stage, graceful, tutu, dramatic lighting, performance"]
                ],
                inputs=[pose_type, prompt],
                outputs=[pose_display, output_display],
                fn=process_generation,
                cache_examples=False
            )

            # Connect components
            pose_type.change(generate_pose_image, inputs=[pose_type], outputs=[pose_display])
            generate_btn.click(
                process_generation,
                inputs=[pose_type, prompt, guidance_scale, conditioning_scale],
                outputs=[pose_display, output_display]
            )

            # Initialize with default pose
            demo.load(generate_pose_image, inputs=[pose_type], outputs=[pose_display])

        return demo

# ============================================
# 4. RUN DEMO
# ============================================

print("\n" + "="*50)
print("SETTING UP DEMO")
print("="*50)

# Initialize generator
generator = PoseToPersonGenerator()

# Test generation with a sample
print("\nTesting generation with sample pose...")
sample_pose = generator.pose_templates["T-pose"]
sample_prompt = "a professional business person in suit, high quality photo"

generated_image = generator.generate_from_pose(sample_pose, sample_prompt)[0]

# Display results
fig, axes = plt.subplots(1, 2, figsize=(12, 6))
axes[0].imshow(sample_pose)
axes[0].set_title('Input Pose Skeleton', fontsize=14, fontweight='bold')
axes[0].axis('off')

axes[1].imshow(generated_image)
axes[1].set_title('Generated Person', fontsize=14, fontweight='bold')
axes[1].axis('off')

plt.suptitle('ControlNet: Pose â†’ Person Generation', fontsize=16, fontweight='bold')
plt.tight_layout()
plt.show()

# ============================================
# 5. GRADIO INTERFACE
# ============================================

print("\n" + "="*50)
print("LAUNCHING INTERACTIVE DEMO")
print("="*50)
print("\nThe Gradio interface will appear below or in a separate link.")

# Create and launch interface
demo = generator.create_demo_interface()
demo.launch(share=True, debug=False)

print("\n" + "="*50)
print("DEMO SUMMARY")
print("="*50)
print("\nâœ… Complete ControlNet Pose-to-Person Pipeline")
print("âœ… Includes:")
print("   - Synthetic dataset creation")
print("   - ControlNet training setup (commented for speed)")
print("   - Pretrained model loading")
print("   - Interactive Gradio demo")
print("   - Sample generation visualization")
print("\nðŸŽ¯ To train the model, uncomment the training code.")
print("ðŸŽ¯ The demo allows you to:")
print("   - Select different pose templates")
print("   - Enter custom prompts")
print("   - Adjust generation parameters")
print("   - See real-time results")

